#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2022/02/08 3:44 PM
# @Author  : yommi
# @Site    :
# @File    : logic_linkkf
# @Software: PyCharm
import json
import os
import random
import re
import sys
import time
import traceback
import urllib
from datetime import datetime
from urllib.parse import urlparse

# third-party
import requests
from bs4 import BeautifulSoup

# third-party
from flask import jsonify, render_template, request
from support.expand.ffmpeg import SupportFfmpeg

# sjva ê³µìš©
from framework import db, path_data, scheduler
from lxml import html
from .mod_base import AnimeModuleBase
from requests_cache import CachedSession

# GDM Integration
try:
    from gommi_downloader_manager.mod_queue import ModuleQueue
except ImportError:
    ModuleQueue = None

# cloudscraperëŠ” lazy importë¡œ ì²˜ë¦¬
import cloudscraper

from anime_downloader.lib.ffmpeg_queue_v1 import FfmpegQueue, FfmpegQueueEntity
from anime_downloader.lib.util import Util
from .mod_ohli24 import LogicOhli24

# íŒ¨í‚¤ì§€
# from .plugin import P
from anime_downloader.setup import *

# from linkkf.model import ModelLinkkfProgram

# from linkkf.model import ModelLinkkfProgram

# from tool_base import d


logger = P.logger
name = "linkkf"


class LogicLinkkf(AnimeModuleBase):
    current_headers = None
    current_data = None
    referer = None
    download_queue = None
    download_thread = None
    current_download_count = 0
    _scraper = None  # cloudscraper ì‹±ê¸€í†¤
    queue = None  # í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ í ê´€ë¦¬

    cache_path = os.path.dirname(__file__)

    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/71.0.3578.98 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "",
    }
    useragent = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, "
        "like Gecko) Chrome/96.0.4664.110 Whale/3.12.129.46 Safari/537.36"
    }

    def __init__(self, P):
        super(LogicLinkkf, self).__init__(P, setup_default=self.db_default, name=name, first_menu='setting', scheduler_desc="linkkf ìë™ ë‹¤ìš´ë¡œë“œ")
        # self.queue = None  # ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ ì´ˆê¸°í™” ì œê±° (í´ë˜ìŠ¤ ë ˆë²¨ ì‚¬ìš©)
        self.db_default = {
            "linkkf_db_version": "1",
            "linkkf_url": "https://linkkf.live",
            f"{self.name}_recent_code": "",
            "linkkf_download_path": os.path.join(path_data, P.package_name, "linkkf"),
            "linkkf_save_path": os.path.join(path_data, P.package_name, "linkkf"),
            "linkkf_auto_make_folder": "True",
            "linkkf_auto_make_season_folder": "True",
            "linkkf_finished_insert": "[ì™„ê²°]",
            "linkkf_max_ffmpeg_process_count": "2",
            f"{self.name}_max_download_count": "2",
            f"{self.name}_quality": "720p",
            "linkkf_order_desc": "False",
            "linkkf_auto_start": "False",
            "linkkf_interval": "* 5 * * *",
            "linkkf_auto_mode_all": "False",
            "linkkf_auto_code_list": "all",
            "linkkf_current_code": "",
            "linkkf_uncompleted_auto_enqueue": "False",
            "linkkf_image_url_prefix_series": "",
            "linkkf_image_url_prefix_episode": "",
            "linkkf_download_method": "ffmpeg",  # ffmpeg, ytdlp, aria2c
            "linkkf_download_threads": "16",     # yt-dlp/aria2c ë³‘ë ¬ ì“°ë ˆë“œ ìˆ˜
            # ì•Œë¦¼ ì„¤ì •
            "linkkf_notify_enabled": "False",
            "linkkf_discord_webhook_url": "",
            "linkkf_telegram_bot_token": "",
            "linkkf_telegram_chat_id": "",
        }
        # default_route_socketio(P, self)
        self.web_list_model = ModelLinkkfItem
        default_route_socketio_module(self, attach="/setting")
        self.current_data = None


    def process_ajax(self, sub, req):
        try:
            if sub == "analysis":
                # code = req.form['code']
                code = request.form["code"]

                wr_id = request.form.get("wr_id", None)
                bo_table = request.form.get("bo_table", None)
                data = []
                # print(code)
                # logger.info("code::: %s", code)
                P.ModelSetting.set("linkkf_current_code", code)
                data = self.get_series_info(code)
                self.current_data = data
                return jsonify({"ret": "success", "data": data, "code": code})
            elif sub == "anime_list":
                data = []
                cate = request.form["type"]
                page = request.form["page"]

                data = self.get_anime_info(cate, page)
                # self.current_data = data
                return jsonify(
                    {"ret": "success", "cate": cate, "page": page, "data": data}
                )
            elif sub == "screen_movie_list":
                try:
                    # logger.debug("request:::> %s", request.form["page"])
                    page = request.form["page"]
                    data = self.get_screen_movie_info(page)
                    dummy_data = {"ret": "success", "data": data}
                    return jsonify(data)
                except Exception as e:
                    logger.error(f"Exception: {str(e)}")
                    logger.error(traceback.format_exc())
            elif sub == "complete_list":
                pass
            elif sub == "search":
                data = []
                # cate = request.form["type"]
                # page = request.form["page"]
                cate = request.form["type"]
                query = request.form["query"]
                page = request.form["page"]

                data = self.get_search_result(query, page, cate)
                # self.current_data = data
                return jsonify(
                    {
                        "ret": "success",
                        "cate": cate,
                        "page": page,
                        "query": query,
                        "data": data,
                    }
                )
            elif sub == "add_queue":
                logger.info("========= add_queue START =========")
                logger.debug("linkkf add_queue routine ===============")
                ret = {}
                try:
                    form_data = request.form.get("data")
                    if not form_data:
                        logger.error(f"No data in form. Form keys: {list(request.form.keys())}")
                        ret["ret"] = "error"
                        ret["log"] = "No data received"
                        return jsonify(ret)
                    info = json.loads(form_data)
                    logger.info(f"info:: {info}")
                    ret["ret"] = self.add(info)
                except Exception as e:
                    logger.error(f"add_queue error: {e}")
                    logger.error(traceback.format_exc())
                    ret["ret"] = "error"
                    ret["log"] = str(e)
            elif sub == "add_queue_checked_list":
                # ì„ íƒëœ ì—í”¼ì†Œë“œ ì¼ê´„ ì¶”ê°€ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬)
                import threading
                from flask import current_app
                
                logger.info("========= add_queue_checked_list START =========")
                ret = {"ret": "success", "message": "ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¶”ê°€ ì¤‘..."}
                try:
                    form_data = request.form.get("data")
                    if not form_data:
                        ret["ret"] = "error"
                        ret["log"] = "No data received"
                        return jsonify(ret)
                    
                    episode_list = json.loads(form_data)
                    logger.info(f"Received {len(episode_list)} episodes to add in background")
                    
                    # Flask app ì°¸ì¡° ì €ì¥ (ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©)
                    app = current_app._get_current_object()
                    
                    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì¶”ê°€ ì‘ì—… ìˆ˜í–‰
                    def add_episodes_background(flask_app, downloader_self, episodes):
                        added = 0
                        skipped = 0
                        with flask_app.app_context():
                            for episode_info in episodes:
                                try:
                                    result = downloader_self.add(episode_info)
                                    if result in ["enqueue_db_append", "enqueue_db_exist"]:
                                        added += 1
                                        logger.debug(f"Added episode {episode_info.get('_id')}")
                                    else:
                                        skipped += 1
                                        logger.debug(f"Skipped episode {episode_info.get('_id')}: {result}")
                                except Exception as ep_err:
                                    logger.error(f"Error adding episode: {ep_err}")
                                    skipped += 1
                            
                            logger.info(f"add_queue_checked_list completed: added={added}, skipped={skipped}")
                    
                    thread = threading.Thread(
                        target=add_episodes_background,
                        args=(app, self, episode_list)
                    )
                    thread.daemon = True
                    thread.start()
                    
                    ret["count"] = len(episode_list)
                    
                except Exception as e:
                    logger.error(f"add_queue_checked_list error: {e}")
                    logger.error(traceback.format_exc())
                    ret["ret"] = "error"
                    ret["log"] = str(e)
                return jsonify(ret)
            elif sub == "add_sub_queue_checked_list":
                # ì„ íƒëœ ì—í”¼ì†Œë“œ ìë§‰ë§Œ ì¼ê´„ ë‹¤ìš´ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬)
                import threading
                from flask import current_app
                
                logger.info("========= add_sub_queue_checked_list START =========")
                ret = {"ret": "success", "message": "ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë§‰ ë‹¤ìš´ë¡œë“œ ì¤‘..."}
                try:
                    form_data = request.form.get("data")
                    if not form_data:
                        ret["ret"] = "error"
                        ret["log"] = "No data received"
                        return jsonify(ret)
                    
                    episode_list = json.loads(form_data)
                    logger.info(f"Received {len(episode_list)} episodes to download subtitles in background")
                    
                    # Flask app ì°¸ì¡° ì €ì¥
                    app = current_app._get_current_object()
                    
                    def download_subtitles_background(flask_app, episode_list):
                        added = 0
                        skipped = 0
                        with flask_app.app_context():
                            for episode_info in episode_list:
                                try:
                                    # LinkkfQueueEntityë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ URL ì¶”ì¶œ (prepare_extra í™œìš©)
                                    entity = LinkkfQueueEntity(P, self, episode_info)
                                    entity.prepare_extra()
                                    
                                    if entity.vtt:
                                        # ìë§‰ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
                                        # entity.filepathëŠ” prepare_extraì—ì„œ ì„¤ì •ë¨ (ê¸°ë³¸ ì €ì¥ ê²½ë¡œ + íŒŒì¼ëª…)
                                        res = Util.download_subtitle(entity.vtt, entity.filepath, headers=entity.headers)
                                        if res:
                                            added += 1
                                            logger.debug(f"Downloaded subtitle for {episode_info.get('title')}")
                                        else:
                                            skipped += 1
                                            logger.info(f"Failed to download subtitle for {episode_info.get('title')}")
                                    else:
                                        skipped += 1
                                        logger.info(f"No subtitle found for {episode_info.get('title')}")
                                except Exception as e:
                                    logger.error(f"Error in download_subtitles_background for one episode: {e}")
                                    skipped += 1
                            
                            logger.info(f"add_sub_queue_checked_list completed: downloaded={added}, skipped={skipped}")

                    thread = threading.Thread(
                        target=download_subtitles_background,
                        args=(app, episode_list)
                    )
                    thread.daemon = True
                    thread.start()
                    
                    ret["count"] = len(episode_list)
                except Exception as e:
                    logger.error(f"add_sub_queue_checked_list error: {e}")
                    logger.error(traceback.format_exc())
                    ret["ret"] = "error"
                    ret["log"] = str(e)
                return jsonify(ret)
            elif sub == "web_list":
                ret = ModelLinkkfItem.web_list(req)
                return jsonify(ret)
            elif sub == "db_remove":
                db_id = request.form.get("id")
                if not db_id:
                    return jsonify({"ret": "error", "log": "No ID provided"})
                return jsonify(ModelLinkkfItem.delete_by_id(db_id))

            elif sub == "merge_subtitle":
                # ìë§‰ í•©ì¹˜ê¸° - ffmpegë¡œ SRTë¥¼ MP4ì— ì‚½ì…
                import subprocess
                import shutil
                
                db_id = request.form.get("id")
                if not db_id:
                    return jsonify({"ret": "error", "message": "No ID provided"})
                
                try:
                    db_item = ModelLinkkfItem.get_by_id(int(db_id))
                    if not db_item:
                        return jsonify({"ret": "error", "message": "Item not found"})
                    
                    mp4_path = db_item.filepath
                    if not mp4_path or not os.path.exists(mp4_path):
                        return jsonify({"ret": "error", "message": f"MP4 file not found: {mp4_path}"})
                    
                    # SRT íŒŒì¼ ê²½ë¡œ (MP4ì™€ ë™ì¼ ê²½ë¡œì— .srt í™•ì¥ì)
                    srt_path = os.path.splitext(mp4_path)[0] + ".srt"
                    if not os.path.exists(srt_path):
                        return jsonify({"ret": "error", "message": f"SRT file not found: {srt_path}"})
                    
                    # ì¶œë ¥ íŒŒì¼: *_subed.mp4
                    base_name = os.path.splitext(mp4_path)[0]
                    output_path = f"{base_name}_subed.mp4"
                    
                    # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë®ì–´ì“°ê¸° ì „ í™•ì¸
                    if os.path.exists(output_path):
                        os.remove(output_path)
                    
                    # ffmpeg ëª…ë ¹ì–´: ìë§‰ì„ soft embed (mov_text ì½”ë±)
                    # -i mp4 -i srt -c:v copy -c:a copy -c:s mov_text output
                    ffmpeg_cmd = [
                        "ffmpeg", "-y",
                        "-i", mp4_path,
                        "-i", srt_path,
                        "-c:v", "copy",
                        "-c:a", "copy",
                        "-c:s", "mov_text",
                        "-metadata:s:s:0", "language=kor",
                        output_path
                    ]
                    
                    logger.info(f"[Merge Subtitle] Running ffmpeg: {' '.join(ffmpeg_cmd)}")
                    result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, timeout=300)
                    
                    if result.returncode != 0:
                        logger.error(f"ffmpeg error: {result.stderr}")
                        return jsonify({"ret": "error", "message": f"ffmpeg failed: {result.stderr[-200:]}"})
                    
                    if not os.path.exists(output_path):
                        return jsonify({"ret": "error", "message": "Output file was not created"})
                    
                    output_size = os.path.getsize(output_path)
                    logger.info(f"[Merge Subtitle] Created: {output_path} ({output_size} bytes)")
                    
                    return jsonify({
                        "ret": "success", 
                        "message": f"ìë§‰ í•©ì¹¨ ì™„ë£Œ!",
                        "output_file": os.path.basename(output_path),
                        "output_size": output_size
                    })
                    
                except subprocess.TimeoutExpired:
                    return jsonify({"ret": "error", "message": "ffmpeg timeout (5ë¶„ ì´ˆê³¼)"})
                except Exception as e:
                    logger.error(f"merge_subtitle error: {e}")
                    logger.error(traceback.format_exc())
                    return jsonify({"ret": "error", "message": str(e)})

            elif sub == "get_playlist":
                # í˜„ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì—ì„œ ë‹¤ìŒ ì—í”¼ì†Œë“œë“¤ ì°¾ê¸°
                try:
                    file_path = request.args.get("path", "")
                    if not file_path or not os.path.exists(file_path):
                        return jsonify({"error": "File not found", "playlist": [], "current_index": 0}), 404
                    
                    # ë³´ì•ˆ ì²´í¬
                    download_path = P.ModelSetting.get("linkkf_download_path")
                    if not file_path.startswith(download_path):
                        return jsonify({"error": "Access denied", "playlist": [], "current_index": 0}), 403
                    
                    folder = os.path.dirname(file_path)
                    current_file = os.path.basename(file_path)
                    
                    # íŒŒì¼ëª…ì—ì„œ SxxExx íŒ¨í„´ ì¶”ì¶œ
                    ep_match = re.search(r'\.S(\d+)E(\d+)\.', current_file, re.IGNORECASE)
                    if not ep_match:
                        # íŒ¨í„´ ì—†ìœ¼ë©´ í˜„ì¬ íŒŒì¼ë§Œ ë°˜í™˜
                        return jsonify({
                            "playlist": [{"path": file_path, "name": current_file}],
                            "current_index": 0
                        })
                    
                    current_season = int(ep_match.group(1))
                    current_episode = int(ep_match.group(2))
                    
                    # ê°™ì€ í´ë”ì˜ ëª¨ë“  mp4 íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
                    all_files = []
                    for f in os.listdir(folder):
                        if f.endswith('.mp4'):
                            match = re.search(r'\.S(\d+)E(\d+)\.', f, re.IGNORECASE)
                            if match:
                                s = int(match.group(1))
                                e = int(match.group(2))
                                all_files.append({
                                    "path": os.path.join(folder, f),
                                    "name": f,
                                    "season": s,
                                    "episode": e
                                })
                    
                    # ì‹œì¦Œ/ì—í”¼ì†Œë“œ ìˆœìœ¼ë¡œ ì •ë ¬
                    all_files.sort(key=lambda x: (x["season"], x["episode"]))
                    
                    # í˜„ì¬ ì—í”¼ì†Œë“œ ì´ìƒì¸ ê²ƒë§Œ í•„í„°ë§ (í˜„ì¬ + ë‹¤ìŒ ì—í”¼ì†Œë“œë“¤)
                    playlist = []
                    current_index = 0
                    for i, f in enumerate(all_files):
                        if f["season"] == current_season and f["episode"] >= current_episode:
                            entry = {"path": f["path"], "name": f["name"]}
                            if f["episode"] == current_episode:
                                current_index = len(playlist)
                            playlist.append(entry)
                    
                    logger.info(f"Linkkf Playlist: {len(playlist)} items, current_index: {current_index}")
                    return jsonify({
                        "playlist": playlist,
                        "current_index": current_index
                    })
                    
                except Exception as e:
                    logger.error(f"Get playlist error: {e}")
                    logger.error(traceback.format_exc())
                    return jsonify({"error": str(e), "playlist": [], "current_index": 0}), 500

            elif sub == "stream_video":
                # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° (MP4 íŒŒì¼ ì§ì ‘ ì„œë¹™)
                try:
                    from flask import send_file, Response, make_response
                    import mimetypes
                    
                    file_path = request.args.get("path", "")
                    if not file_path or not os.path.exists(file_path):
                        return "File not found", 404
                    
                    # ë³´ì•ˆ ì²´í¬: ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                    download_path = P.ModelSetting.get("linkkf_download_path")
                    if not file_path.startswith(download_path):
                        return "Access denied", 403
                        
                    file_size = os.path.getsize(file_path)
                    range_header = request.headers.get('Range', None)
                    
                    if not range_header:
                        return send_file(file_path, mimetype='video/mp4', as_attachment=False)
                    
                    # Range Request ì²˜ë¦¬ (seeking ì§€ì›)
                    byte1, byte2 = 0, None
                    m = re.search('(\d+)-(\d*)', range_header)
                    if m:
                        g = m.groups()
                        byte1 = int(g[0])
                        if g[1]:
                            byte2 = int(g[1])
                    
                    if byte2 is None:
                        byte2 = file_size - 1
                    
                    length = byte2 - byte1 + 1
                    
                    with open(file_path, 'rb') as f:
                        f.seek(byte1)
                        data = f.read(length)
                    
                    rv = Response(data, 206, mimetype='video/mp4', content_type='video/mp4', direct_passthrough=True)
                    rv.headers.add('Content-Range', 'bytes {0}-{1}/{2}'.format(byte1, byte2, file_size))
                    rv.headers.add('Accept-Ranges', 'bytes')
                    return rv
                except Exception as e:
                    logger.error(f"Stream video error: {e}")
                    logger.error(traceback.format_exc())
                    return jsonify({"error": str(e)}), 500

            # ë§¤ì¹˜ë˜ëŠ” subê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
            if sub == "browse_dir":
                try:
                    path = request.form.get("path", "")
                    if not path or not os.path.exists(path):
                        path = P.ModelSetting.get("linkkf_download_path") or os.path.expanduser("~")
                    path = os.path.abspath(path)
                    if not os.path.isdir(path):
                        path = os.path.dirname(path)
                    directories = []
                    try:
                        for item in sorted(os.listdir(path)):
                            item_path = os.path.join(path, item)
                            if os.path.isdir(item_path) and not item.startswith('.'):
                                directories.append({"name": item, "path": item_path})
                    except PermissionError:
                        pass
                    parent = os.path.dirname(path) if path != "/" else None
                    return jsonify({
                        "ret": "success",
                        "current_path": path,
                        "parent_path": parent,
                        "directories": directories
                    })
                except Exception as e:
                    logger.error(f"browse_dir error: {e}")
                    return jsonify({"ret": "error", "error": str(e)}), 500

            elif sub == "test_notification":
                # í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ì „ì†¡
                try:
                    discord_url = P.ModelSetting.get("linkkf_discord_webhook_url")
                    telegram_token = P.ModelSetting.get("linkkf_telegram_bot_token")
                    telegram_chat_id = P.ModelSetting.get("linkkf_telegram_chat_id")
                    
                    if not discord_url and not (telegram_token and telegram_chat_id):
                        return jsonify({"ret": "error", "msg": "Discord Webhook URL ë˜ëŠ” Telegram ì„¤ì •ì„ ì…ë ¥í•˜ì„¸ìš”."})
                    
                    test_message = "ğŸ”” **í…ŒìŠ¤íŠ¸ ì•Œë¦¼**\nLinkkf ì•Œë¦¼ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n\nì•Œë¦¼ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜ì‹ ë˜ê³  ìˆìŠµë‹ˆë‹¤."
                    sent_to = []
                    
                    if discord_url:
                        self.send_discord_notification(discord_url, "í…ŒìŠ¤íŠ¸", test_message)
                        sent_to.append("Discord")
                    
                    if telegram_token and telegram_chat_id:
                        self.send_telegram_notification(telegram_token, telegram_chat_id, test_message)
                        sent_to.append("Telegram")
                    
                    return jsonify({"ret": "success", "msg": f"{', '.join(sent_to)}ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ!"})
                except Exception as e:
                    logger.error(f"test_notification error: {e}")
                    return jsonify({"ret": "error", "msg": str(e)})

            return super().process_ajax(sub, req)

        except Exception as e:
            P.logger.error(f"Exception: {str(e)}")
            P.logger.error(traceback.format_exc())
            return jsonify({"ret": "error", "log": str(e)})

    def process_command(self, command, arg1, arg2, arg3, req):
        try:
            if command == "list":
                # 1. ìì²´ í ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                ret = self.queue.get_entity_list() if self.queue else []
                
                # 2. GDM íƒœìŠ¤í¬ ê°€ì ¸ì˜¤ê¸° (ì„¤ì¹˜ëœ ê²½ìš°)
                try:
                    from gommi_downloader_manager.mod_queue import ModuleQueue
                    if ModuleQueue:
                        gdm_tasks = ModuleQueue.get_all_downloads()
                        # ì´ ëª¨ë“ˆ(linkkf)ì´ ì¶”ê°€í•œ ì‘ì—…ë§Œ í•„í„°ë§
                        linkkf_tasks = [t for t in gdm_tasks if t.caller_plugin == f"{P.package_name}_{self.name}"]
                        
                        for task in linkkf_tasks:
                            # í…œí”Œë¦¿ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                            gdm_item = self._convert_gdm_task_to_queue_item(task)
                            ret.append(gdm_item)
                except Exception as e:
                    logger.debug(f"GDM tasks fetch error: {e}")
                
                return jsonify(ret)
                
            elif command in ["stop", "remove", "cancel"]:
                entity_id = arg1
                if entity_id and str(entity_id).startswith("dl_"):
                    # GDM ì‘ì—… ì²˜ë¦¬
                    try:
                        from gommi_downloader_manager.mod_queue import ModuleQueue
                        if ModuleQueue:
                            if command == "stop" or command == "cancel":
                                task = ModuleQueue.get_download(entity_id)
                                if task:
                                    task.cancel()
                                    return jsonify({"ret": "success", "log": "GDM ì‘ì—…ì„ ì¤‘ì§€í•˜ì˜€ìŠµë‹ˆë‹¤."})
                            elif command == "remove":
                                # GDMì—ì„œ ì‚­ì œ ì²˜ë¦¬ (ëª…ë ¹ì–´ 'delete' ì‚¬ìš©)
                                # process_ajaxì˜ delete ë¡œì§ ì°¸ê³ 
                                class DummyReq:
                                    def __init__(self, id):
                                        self.form = {"id": id}
                                ModuleQueue.process_ajax("delete", DummyReq(entity_id))
                                return jsonify({"ret": "success", "log": "GDM ì‘ì—…ì„ ì‚­ì œí•˜ì˜€ìŠµë‹ˆë‹¤."})
                    except Exception as e:
                        logger.error(f"GDM command error: {e}")
                        return jsonify({"ret": "error", "log": f"GDM ëª…ë ¹ ì‹¤íŒ¨: {e}"})
                
                # ìì²´ í ì²˜ë¦¬
                return super().process_command(command, arg1, arg2, arg3, req)
                
            return super().process_command(command, arg1, arg2, arg3, req)
        except Exception as e:
            logger.error(f"process_command Error: {e}")
            logger.error(traceback.format_exc())
            return jsonify({'ret': 'fail', 'log': str(e)})

    def _convert_gdm_task_to_queue_item(self, task):
        """GDM DownloadTask ê°ì²´ë¥¼ FfmpegQueueEntity.as_dict() í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # ìƒíƒœ ë§µí•‘
        status_kor_map = {
            "pending": "ëŒ€ê¸°ì¤‘",
            "extracting": "ë¶„ì„ì¤‘",
            "downloading": "ë‹¤ìš´ë¡œë“œì¤‘",
            "paused": "ì¼ì‹œì •ì§€",
            "completed": "ì™„ë£Œ",
            "error": "ì‹¤íŒ¨",
            "cancelled": "ì·¨ì†Œë¨"
        }
        
        status_str_map = {
            "pending": "WAITING",
            "extracting": "ANALYZING",
            "downloading": "DOWNLOADING",
            "paused": "PAUSED",
            "completed": "COMPLETED",
            "error": "FAILED",
            "cancelled": "FAILED"
        }
        
        # GDM taskëŠ” as_dict()ë¥¼ ì œê³µí•¨
        t_dict = task.as_dict()
        
        return {
            "entity_id": t_dict["id"],
            "url": t_dict["url"],
            "filename": t_dict["filename"] or t_dict["title"],
            "status_kor": status_kor_map.get(t_dict["status"], "ì•Œìˆ˜ì—†ìŒ"),
            "percent": t_dict["progress"],
            "created_time": t_dict["created_time"],
            "current_speed": t_dict["speed"] or "0 B/s",
            "download_time": t_dict["eta"] or "-",
            "status_str": status_str_map.get(t_dict["status"], "WAITING"),
            "idx": t_dict["id"],
            "callback_id": "linkkf",
            "start_time": t_dict["start_time"] or t_dict["created_time"],
            "save_fullpath": t_dict["filepath"],
            "duration_str": "GDM",
            "current_pf_count": 0,
            "duration": "-",
            "current_duration": "-",
            "current_bitrate": "-",
            "max_pf_count": 0,
            "is_gdm": True
        }

    def plugin_callback(self, data):
        """
        GDM ëª¨ë“ˆë¡œë¶€í„° ë‹¤ìš´ë¡œë“œ ìƒíƒœ ì—…ë°ì´íŠ¸ ìˆ˜ì‹ 
        data = {
            'callback_id': self.callback_id,
            'status': self.status,
            'filepath': self.filepath,
            'filename': os.path.basename(self.filepath) if self.filepath else '',
            'error': self.error_message
        }
        """
        try:
            callback_id = data.get('callback_id')
            status = data.get('status')
            
            logger.info(f"[Linkkf] Received GDM callback: id={callback_id}, status={status}")
            
            # DB ìƒíƒœ ì—…ë°ì´íŠ¸
            if callback_id:
                from framework import F
                with F.app.app_context():
                    db_item = ModelLinkkfItem.get_by_linkkf_id(callback_id)
                    if db_item:
                        if status == "completed":
                            db_item.status = "completed"
                            db_item.completed_time = datetime.now()
                            # ê²½ë¡œ ì •ê·œí™” í›„ ì €ì¥
                            new_filepath = data.get('filepath')
                            if new_filepath:
                                db_item.filepath = os.path.normpath(new_filepath)
                            db_item.save()
                            logger.info(f"[Linkkf] Successfully updated DB item {db_item.id} (Linkkf ID: {callback_id}) to COMPLETED via GDM callback")
                            logger.info(f"[Linkkf] Final filepath in DB: {db_item.filepath}")
                            
                            # ì•Œë¦¼ ì „ì†¡ (í•„ìš” ì‹œ)
                            # self.socketio_callback("list_refresh", "")
                        elif status == "error":
                            # í•„ìš” ì‹œ ì—ëŸ¬ ì²˜ë¦¬
                            pass
        except Exception as e:
            logger.error(f"[Linkkf] Callback processing error: {e}")
            logger.error(traceback.format_exc())

    def socketio_callback(self, refresh_type, data):
        """
        socketioë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ì— ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ì†¡
        refresh_type: 'add', 'status', 'last' ë“±
        data: entity.as_dict() ë°ì´í„°
        """
        logger.info(f">>> socketio_callback called: {refresh_type}, {data.get('percent', 'N/A')}%")
        try:
            from framework import socketio
            
            # FlaskFarmì˜ ê¸°ì¡´ íŒ¨í„´: /framework namespaceë¡œ emit
            # queue í˜ì´ì§€ì˜ ì†Œì¼“ì´ ì´ ë©”ì‹œì§€ë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬
            namespace = f"/{P.package_name}/{self.name}/queue"
            
            # ë¨¼ì € queueì— ì§ì ‘ emit (ê¸°ì¡´ ë°©ì‹)
            socketio.emit(refresh_type, data, namespace=namespace)
            
            # /framework namespaceë¡œë„ notify ì´ë²¤íŠ¸ ì „ì†¡
            notify_data = {
                "type": "success",
                "msg": f"ë‹¤ìš´ë¡œë“œì¤‘ {data.get('percent', 0)}% - {data.get('filename', '')}",
            }
            socketio.emit("notify", notify_data, namespace="/framework")
            logger.info(f">>> socketio.emit completed to /framework")
                
        except Exception as e:
            logger.error(f"socketio_callback error: {e}")

    @staticmethod
    def _extract_cat1_urls(html_content):
        """cat1 = [...] íŒ¨í„´ì—ì„œ URL ëª©ë¡ ì¶”ì¶œ (ì¤‘ë³µ ì½”ë“œ ì œê±°ìš© í—¬í¼)"""
        regex = r"cat1 = [^\[]*([^\]]*)"
        cat_match = re.findall(regex, html_content)
        if not cat_match:
            return []
        url_regex = r"\"([^\"]*)\""
        return re.findall(url_regex, cat_match[0])

    @staticmethod
    def get_html(url, cached=False, timeout=10):
        try:
            if LogicLinkkf.referer is None:
                LogicLinkkf.referer = f"{P.ModelSetting.get('linkkf_url')}"

            return LogicLinkkf.get_html_cloudflare(url, timeout=timeout)

        except Exception as e:
            logger.error("Exception:%s", e)
            logger.error(traceback.format_exc())

    @staticmethod
    def get_html_cloudflare(url, cached=False, timeout=10):
        """Cloudflare ë³´í˜¸ ìš°íšŒë¥¼ ìœ„í•œ HTTP ìš”ì²­ ( Zendriver Daemon -> Subprocess -> Camoufox -> Scraper ìˆœ)"""
        start_time = time.time()
        
        # 0. Referer ì„¤ì •
        if LogicLinkkf.referer is None:
            LogicLinkkf.referer = f"{P.ModelSetting.get('linkkf_url')}"
        
        LogicLinkkf.headers["Referer"] = LogicLinkkf.referer or ""

        # 1. Zendriver Daemon ì‹œë„ (ìµœìš°ì„ )
        try:
            if LogicOhli24.is_zendriver_daemon_running():
                logger.info(f"[Linkkf] Trying Zendriver Daemon: {url}")
                daemon_res = LogicOhli24.fetch_via_daemon(url, timeout=30, headers=LogicLinkkf.headers)
                if daemon_res.get("success") and daemon_res.get("html"):
                    elapsed = time.time() - start_time
                    logger.info(f"[Linkkf] Daemon success in {elapsed:.2f}s")
                    return daemon_res["html"]
        except Exception as e:
            logger.warning(f"[Linkkf] Daemon error: {e}")

        # 2. Scraper ì‹œë„ (ê¸°ë³¸)
        try:
            if LogicLinkkf._scraper is None:
                LogicLinkkf._scraper = cloudscraper.create_scraper(
                    delay=10,
                    browser={"custom": "linkkf"},
                )
            
            user_agents_list = [
                "Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36",
            ]
            LogicLinkkf.headers["User-Agent"] = random.choice(user_agents_list)
            
            response = LogicLinkkf._scraper.get(url, headers=LogicLinkkf.headers, timeout=timeout)
            
            # ì±Œë¦°ì§€ í˜ì´ì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ì½˜í…ì¸ ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            content = response.text
            if "Cloudflare" not in content or "video-player" in content or "iframe" in content:
                return content
            
            logger.warning("[Linkkf] Scraper returned challenge page, falling back to browser...")
        except Exception as e:
            logger.warning(f"[Linkkf] Scraper error: {e}")

        # 3. Zendriver Subprocess Fallback
        try:
            if LogicOhli24.ensure_zendriver_installed():
                logger.info(f"[Linkkf] Trying Zendriver subprocess: {url}")
                script_path = os.path.join(os.path.dirname(__file__), "lib", "zendriver_ohli24.py")
                cmd = [sys.executable, script_path, url, str(30)]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
                if result.returncode == 0 and result.stdout.strip():
                    zd_result = json.loads(result.stdout.strip())
                    if zd_result.get("success") and zd_result.get("html"):
                        return zd_result["html"]
        except Exception as e:
            logger.warning(f"[Linkkf] Zendriver fallback error: {e}")

        # 4. Camoufox Fallback
        try:
            logger.info(f"[Linkkf] Trying Camoufox fallback: {url}")
            script_path = os.path.join(os.path.dirname(__file__), "lib", "camoufox_ohli24.py")
            result = subprocess.run([sys.executable, script_path, url, str(30)], capture_output=True, text=True, timeout=60)
            if result.returncode == 0 and result.stdout.strip():
                cf_result = json.loads(result.stdout.strip())
                if cf_result.get("success") and cf_result.get("html"):
                    return cf_result["html"]
        except Exception as e:
            logger.warning(f"[Linkkf] Camoufox fallback error: {e}")

        return ""

    @staticmethod
    def add_whitelist(*args):
        ret = {}

        logger.debug(f"args: {args}")
        try:
            if len(args) == 0:
                code = str(LogicLinkkf.current_data["code"])
            else:
                code = str(args[0])

            logger.debug(f"add_whitelist code: {code}")

            whitelist_program = P.ModelSetting.get("linkkf_auto_code_list")
            # whitelist_programs = [
            #     str(x.strip().replace(" ", ""))
            #     for x in whitelist_program.replace("\n", "|").split("|")
            # ]
            whitelist_programs = [
                str(x.strip()) for x in whitelist_program.replace("\n", "|").split("|")
            ]

            if code not in whitelist_programs:
                whitelist_programs.append(code)
                whitelist_programs = filter(
                    lambda x: x != "", whitelist_programs
                )  # remove blank code
                whitelist_program = "|".join(whitelist_programs)
                entity = (
                    db.session.query(P.ModelSetting)
                    .filter_by(key="linkkf_auto_code_list")
                    .with_for_update()
                    .first()
                )
                entity.value = whitelist_program
                db.session.commit()
                ret["ret"] = True
                ret["code"] = code
                if len(args) == 0:
                    return LogicLinkkf.current_data
                else:
                    return ret
            else:
                ret["ret"] = False
                ret["log"] = "ì´ë¯¸ ì¶”ê°€ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error("Exception:%s", e)
            logger.error(traceback.format_exc())
            ret["ret"] = False
            ret["log"] = str(e)
        return ret

    def setting_save_after(self, change_list=None):
        if self.queue.get_max_ffmpeg_count() != P.ModelSetting.get_int(
            "linkkf_max_ffmpeg_process_count"
        ):
            self.queue.set_max_ffmpeg_count(
                P.ModelSetting.get_int("linkkf_max_ffmpeg_process_count")
            )

    @staticmethod
    def extract_video_url_from_playid(playid_url):
        """
        linkkf.liveì˜ playid URLì—ì„œ ì‹¤ì œ ë¹„ë””ì˜¤ URL(m3u8)ê³¼ ìë§‰ URL(vtt)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        ì˜ˆì‹œ:
        - playid_url: https://linkkf.live/playid/403116/?server=12&slug=11
        - iframe: https://play.sub3.top/r2/play.php?id=n8&url=403116s11
        - m3u8: https://n8.hlz3.top/403116s11/index.m3u8
        
        Returns:
            (video_url, referer_url, vtt_url)
        """
        video_url = None
        referer_url = None
        vtt_url = None
        
        try:
            logger.info(f"Extracting video URL from: {playid_url}")
            
            # Step 1: playid í˜ì´ì§€ì—ì„œ iframe src ì¶”ì¶œ (cloudscraper ì‚¬ìš©)
            html_content = LogicLinkkf.get_html(playid_url)
            if not html_content:
                logger.error(f"Failed to fetch playid page: {playid_url}")
                return None, None, None
                
            soup = BeautifulSoup(html_content, "html.parser")
            
            # iframe ì°¾ê¸° (ê´‘ê³  iframe ì œì™¸ë¥¼ ìœ„í•´ idë‚˜ src íŒ¨í„´ ê°•ì¡°)
            iframe = soup.select_one("iframe#video-player-iframe")
            if not iframe:
                iframe = soup.select_one("iframe[src*='play.sub']")
            if not iframe:
                iframe = soup.select_one("iframe[src*='play.php']")
            
            # fallback if strictly needed but skip ad domains
            if not iframe:
                all_iframes = soup.select("iframe")
                for f in all_iframes:
                    src = f.get("src", "")
                    if any(x in src for x in ["googletag", "googlead", "adsystem", "cloud.google"]): 
                        continue
                    if src.startswith("http"):
                        iframe = f
                        break
            
            if iframe and iframe.get("src"):
                iframe_src = iframe.get("src")
                # HTML entity decoding (&#038; -> &, &amp; -> &, etc.)
                import html as html_lib
                iframe_src = html_lib.unescape(iframe_src)
                
                logger.info(f"Found player iframe: {iframe_src}")
                
                # Step 2: iframe í˜ì´ì§€ì—ì„œ m3u8 URLê³¼ vtt URL ì¶”ì¶œ
                iframe_content = LogicLinkkf.get_html(iframe_src)
                if not iframe_content:
                    logger.error(f"Failed to fetch iframe content: {iframe_src}")
                    return None, iframe_src, None
                
                # m3u8 URL íŒ¨í„´ ì°¾ê¸° (ë” ì •ë°€í•˜ê²Œ)
                # íŒ¨í„´ 1: url: 'https://...m3u8' ë˜ëŠ” url: "https://...m3u8"
                m3u8_pattern = re.compile(r"url:\s*['\"]([^'\"]*\.m3u8[^'\"]*)['\"]")
                m3u8_match = m3u8_pattern.search(iframe_content)
                
                # íŒ¨í„´ 2: <source src="https://...m3u8">
                if not m3u8_match:
                    source_pattern = re.compile(r"<source[^>]+src=['\"]([^'\"]*\.m3u8[^'\"]*)['\"]", re.IGNORECASE)
                    m3u8_match = source_pattern.search(iframe_content)
                
                # íŒ¨í„´ 3: var src = '...m3u8'
                if not m3u8_match:
                    src_pattern = re.compile(r"src\s*=\s*['\"]([^'\"]*\.m3u8[^'\"]*)['\"]", re.IGNORECASE)
                    m3u8_match = src_pattern.search(iframe_content)

                # íŒ¨í„´ 4: Artplayer ì „ìš© ë” ë„“ì€ ë²”ìœ„
                if not m3u8_match:
                    art_pattern = re.compile(r"url\s*:\s*['\"]([^'\"]+)['\"]")
                    matches = art_pattern.findall(iframe_content)
                    for m in matches:
                        if ".m3u8" in m:
                            video_url = m
                            break
                    if video_url:
                        logger.info(f"Extracted m3u8 via Artplayer pattern: {video_url}")

                if m3u8_match and not video_url:
                    video_url = m3u8_match.group(1)
                
                if video_url:
                    # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬ (ì˜ˆ: cache/...)
                    if video_url.startswith('cache/') or video_url.startswith('/cache/'):
                        from urllib.parse import urljoin
                        video_url = urljoin(iframe_src, video_url)
                    logger.info(f"Extracted m3u8 URL: {video_url}")
                else:
                    logger.warning(f"m3u8 URL not found in iframe for: {playid_url}")
                    # HTML ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ë¡œê¹…
                    snippet = iframe_content.replace('\n', ' ')
                    logger.debug(f"Iframe Content snippet (500 chars): {snippet[:500]}...")
                    # 'cache/' ê°€ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸
                    if 'cache/' in iframe_content:
                        logger.debug("Found 'cache/' keyword in iframe content but regex failed. Inspection required.")
                
                # VTT ìë§‰ URL ì¶”ì¶œ
                # VTT ìë§‰ URL ì¶”ì¶œ (íŒ¨í„´ 1: generic src)
                vtt_pattern = re.compile(r"['\"]src['\"]?:\s*['\"]([^'\"]*\.vtt)['\"]", re.IGNORECASE)
                vtt_match = vtt_pattern.search(iframe_content)
                
                # íŒ¨í„´ 2: url: '...vtt' (Artplayer ë“±)
                if not vtt_match:
                    vtt_pattern = re.compile(r"url:\s*['\"]([^'\"]*\.vtt[^'\"]*)['\"]", re.IGNORECASE)
                    vtt_match = vtt_pattern.search(iframe_content)

                if vtt_match:
                    vtt_url = vtt_match.group(1)
                    if vtt_url.startswith('s/') or vtt_url.startswith('/s/'):
                        from urllib.parse import urljoin
                        vtt_url = urljoin(iframe_src, vtt_url)
                    logger.info(f"Extracted VTT URL: {vtt_url}")
                else:
                    logger.debug("VTT URL not found in iframe content.")
                
                referer_url = iframe_src
            else:
                logger.warning(f"No player iframe found in playid page. HTML snippet: {html_content[:200]}...")
                
        except Exception as e:
            logger.error(f"Error in extract_video_url_from_playid: {e}")
            logger.error(traceback.format_exc())
        
        return video_url, referer_url, vtt_url

    def get_video_url_from_url(url, url2):
        video_url = None
        referer_url = None
        vtt_url = None
        LogicLinkkf.referer = url2
        # logger.info("dx download url : %s , url2 : %s" % (url, url2))
        # logger.debug(LogicLinkkfYommi.referer)

        try:
            if "ani1" in url2:
                # kfani ê³„ì—´ ì²˜ë¦¬ => ë°©ë¬¸í•´ì„œ m3u8ì„ ë°›ì•„ì˜¨ë‹¤.
                logger.debug("ani1 routine=========================")
                LogicLinkkf.referer = "https://linkkf.app"
                # logger.debug(f"url2: {url2}")
                ani1_html = LogicLinkkf.get_html(url2)

                tree = html.fromstring(ani1_html)
                option_url = tree.xpath("//select[@id='server-list']/option[1]/@value")

                # logger.debug(f"option_url:: {option_url}")

                data = LogicLinkkf.get_html(option_url[0])
                # print(type(data))
                regex2 = r'"([^\"]*m3u8)"|<source[^>]+src=\"([^"]+)'

                temp_url = re.findall(regex2, data)[0]
                video_url = ""
                ref = "https://ani1.app"
                for i in temp_url:
                    if i is None:
                        continue
                    video_url = i
                    # video_url = '{1} -headers \'Referer: "{0}"\' -user_agent "Mozilla/5.0 (Windows NT 10.0; Win64;
                    # x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3554.0 Safari/537.36"'.format(ref,
                    # video_url)

                data_tree = html.fromstring(data)
                # print(data_tree.xpath("//video/source/@src"))
                vtt_elem = data_tree.xpath("//track/@src")[0]
                # vtt_elem = data_tree.xpath("//*[contains(@src, '.vtt']")[0]

                # print(vtt_elem)

                match = re.compile(
                    r"<track.+src=\"(?P<vtt_url>.*?.vtt)\"", re.MULTILINE
                ).search(data)

                vtt_url = match.group("vtt_url")

                referer_url = "https://kfani.me/"

            elif "kfani" in url2:
                # kfani ê³„ì—´ ì²˜ë¦¬ => ë°©ë¬¸í•´ì„œ m3u8ì„ ë°›ì•„ì˜¨ë‹¤.
                logger.debug("kfani routine=================================")
                LogicLinkkf.referer = url2
                # logger.debug(f"url2: {url2}")
                data = LogicLinkkf.get_html(url2)
                # logger.info("dx: data", data)
                regex2 = r'"([^\"]*m3u8)"|<source[^>]+src=\"([^"]+)'

                temp_url = re.findall(regex2, data)[0]
                video_url = ""
                ref = "https://kfani.me"
                for i in temp_url:
                    if i is None:
                        continue
                    video_url = i
                    # video_url = '{1} -headers \'Referer: "{0}"\' -user_agent "Mozilla/5.0 (Windows NT 10.0; Win64;
                    # x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3554.0 Safari/537.36"'.format(ref,
                    # video_url)

                # @k45734
                vtt_url = None
                try:
                    _match1 = re.compile(
                        r"<track.+src=\"(?P<vtt_url>.*?.vtt)", re.MULTILINE
                    ).search(data)
                    vtt_url = _match1.group("vtt_url")
                except:
                    _match2 = re.compile(
                        r"url: \'(?P<vtt_url>.*?.vtt)", re.MULTILINE
                    ).search(data)
                    vtt_url = _match2.group("vtt_url")

                logger.info("vtt_url: %s", vtt_url)

                referer_url = url2

            elif "kftv" in url2:
                # kftv ê³„ì—´ ì²˜ë¦¬ => urlì˜ idë¡œ https://yt.kftv.live/getLinkStreamMd5/df6960891d226e24b117b850b44a2290 í˜ì´ì§€
                # ì ‘ì†í•´ì„œ json ë°›ì•„ì˜¤ê³ , jsonì—ì„œ urlì„ ì¶”ì¶œí•´ì•¼í•¨
                if "=" in url2:
                    md5 = urlparse.urlparse(url2).query.split("=")[1]
                elif "embedplay" in url2:
                    md5 = url2.split("/")[-1]
                url3 = "https://yt.kftv.live/getLinkStreamMd5/" + md5
                # logger.info("download url : %s , url3 : %s" % (url, url3))
                data3 = LogicLinkkf.get_html(url3)
                data3dict = json.loads(data3)
                # print(data3dict)
                video_url = data3dict[0]["file"]

            elif "k40chan" in url2:
                # k40chan ê³„ì—´ ì²˜ë¦¬ => ë°©ë¬¸í•´ì„œ m3u8ì„ ë°›ì•„ì˜¨ë‹¤.
                # k45734 ë‹˜ ì†ŒìŠ¤ ë°˜ì˜ (í™•ì¸ì€ ì•ˆí•´ë´„ ì˜ ë™ì‘í• êº¼ë¼ê³  ë¯¿ê³ ,)
                logger.debug("k40chan routine=================================")
                LogicLinkkf.referer = url2
                data = LogicLinkkf.get_html(url2)

                regex2 = r'"([^\"]*m3u8)"|<source[^>]+src=\"([^"]+)'

                temp_url = re.findall(regex2, data)[0]
                video_url = ""
                # ref = "https://kfani.me"
                for i in temp_url:
                    if i is None:
                        continue
                    video_url = i

                match = re.compile(r"<track.+src\=\"(?P<vtt_url>.*?.vtt)").search(data)
                vtt_url = match.group("vtt_url")

                referer_url = url2

            elif "linkkf" in url2:
                logger.debug("linkkf routine")
                # linkkf ê³„ì—´ ì²˜ë¦¬ => URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì˜¤ê³ , í•˜ë‚˜ ê³¨ë¼ ë°©ë¬¸ í•´ì„œ m3u8ì„ ë°›ì•„ì˜¨ë‹¤.
                referer_url = url2
                data2 = LogicLinkkf.get_html(url2)
                # print(data2)
                regex = r"cat1 = [^\[]*([^\]]*)"
                cat = re.findall(regex, data2)[0]
                # logger.info("cat: %s", cat)
                regex = r"\"([^\"]*)\""
                url3s = re.findall(regex, cat)
                url3 = random.choice(url3s)
                # logger.info("url3: %s", url3)
                # logger.info("download url : %s , url3 : %s" % (url, url3))
                if "kftv" in url3:
                    return LogicLinkkf.get_video_url_from_url(url2, url3)
                elif url3.startswith("/"):
                    url3 = urlparse.urljoin(url2, url3)
                    logger.debug(f"url3 = {url3}")
                    LogicLinkkf.referer = url2
                    data3 = LogicLinkkf.get_html(url3)
                    # logger.info('data3: %s', data3)
                    # regex2 = r'"([^\"]*m3u8)"'
                    regex2 = r'"([^\"]*mp4|m3u8)"'
                    video_url = re.findall(regex2, data3)[0]
                    # logger.info('video_url: %s', video_url)
                    referer_url = url3

                else:
                    logger.error("ìƒˆë¡œìš´ ìœ í˜•ì˜ url ë°œìƒ! %s %s %s" % (url, url2, url3))
            elif "kakao" in url2:
                # kakao ê³„ì—´ ì²˜ë¦¬, ì™¸ë¶€ API ì´ìš©
                payload = {"inputUrl": url2}
                kakao_url = (
                    "http://webtool.cusis.net/wp-pages/download-kakaotv-video/video.php"
                )
                data2 = requests.post(
                    kakao_url,
                    json=payload,
                    headers={
                        "referer": "http://webtool.cusis.net/download-kakaotv-video/"
                    },
                ).content
                time.sleep(
                    3
                )  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ë‹¨ì‹œê°„ì— ë„ˆë¬´ ë§ì€ URLì „ì†¡ì„ í•˜ë©´ IPë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.
                url3 = json.loads(data2)
                # logger.info("download url2 : %s , url3 : %s" % (url2, url3))
                video_url = url3
            elif "#V" in url2:  # V íŒ¨í„´ ì¶”ê°€
                logger.debug("#v routine")

                data2 = LogicLinkkf.get_html(url2)

                regex = r"cat1 = [^\[]*([^\]]*)"
                cat = re.findall(regex, data2)[0]
                regex = r"\"([^\"]*)\""
                url3s = re.findall(regex, cat)
                url3 = random.choice(url3s)
                # logger.info("download url : %s , url3 : %s" % (url, url3))
                if "kftv" in url3:
                    return LogicLinkkf.get_video_url_from_url(url2, url3)
                elif url3.startswith("/"):
                    url3 = urlparse.urljoin(url2, url3)
                    LogicLinkkf.referer = url2
                    data3 = LogicLinkkf.get_html(url3)

                    regex2 = r'"([^\"]*mp4)"'
                    video_url = re.findall(regex2, data3)[0]
                else:
                    logger.error("ìƒˆë¡œìš´ ìœ í˜•ì˜ url ë°œìƒ! %s %s %s" % (url, url2, url3))

            elif "#M2" in url2:
                LogicLinkkf.referer = url2
                data2 = LogicLinkkf.get_html(url2)
                # print(data2)

                regex = r"cat1 = [^\[]*([^\]]*)"
                cat = re.findall(regex, data2)[0]
                regex = r"\"([^\"]*)\""
                url3s = re.findall(regex, cat)
                url3 = random.choice(url3s)
                # logger.info("download url : %s , url3 : %s" % (url, url3))
                if "kftv" in url3:
                    return LogicLinkkf.get_video_url_from_url(url2, url3)
                elif url3.startswith("/"):
                    url3 = urlparse.urljoin(url2, url3)
                    LogicLinkkf.referer = url2
                    data3 = LogicLinkkf.get_html(url3)
                    # print("ë‚´ìš©: %s", data3)
                    # logger.info("movie content: %s", data3)
                    # regex2 = r'"([^\"]*m3u8)"'
                    regex2 = r'"([^\"]*mp4)"'
                    video_url = re.findall(regex2, data3)[0]
                else:
                    logger.error("ìƒˆë¡œìš´ ìœ í˜•ì˜ url ë°œìƒ! %s %s %s" % (url, url2, url3))
            elif "ğŸ˜€#i" in url2:
                LogicLinkkf.referer = url2
                data2 = LogicLinkkf.get_html(url2)
                # logger.info(data2)

                regex = r"cat1 = [^\[]*([^\]]*)"
                cat = re.findall(regex, data2)[0]
                regex = r"\"([^\"]*)\""
                url3s = re.findall(regex, cat)
                url3 = random.choice(url3s)
                # logger.info("download url : %s , url3 : %s" % (url, url3))

            elif "#k" in url2:
                data2 = LogicLinkkf.get_html(url2)
                # logger.info(data2)

                regex = r"cat1 = [^\[]*([^\]]*)"
                cat = re.findall(regex, data2)[0]
                regex = r"\"([^\"]*)\""
                url3s = re.findall(regex, cat)
                url3 = random.choice(url3s)
                # logger.info("download url : %s , url3 : %s" % (url, url3))

            elif "#k2" in url2:
                data2 = LogicLinkkf.get_html(url2)
                # logger.info(data2)

                regex = r"cat1 = [^\[]*([^\]]*)"
                cat = re.findall(regex, data2)[0]
                regex = r"\"([^\"]*)\""
                url3s = re.findall(regex, cat)
                url3 = random.choice(url3s)
                # logger.info("download url : %s , url3 : %s" % (url, url3))
            elif "mopipi" in url2:
                LogicLinkkf.referer = url
                data2 = LogicLinkkf.get_html(url2)
                # logger.info(data2)
                match = re.compile(r"src\=\"(?P<video_url>http.*?\.mp4)").search(data2)
                video_url = match.group("video_url")

                match = re.compile(r"src\=\"(?P<vtt_url>http.*?.vtt)").search(data2)
                logger.info("match group: %s", match.group("video_url"))
                vtt_url = match.group("vtt_url")

                # logger.info("download url : %s , url3 : %s" % (url, url3))

            else:
                logger.error("ìƒˆë¡œìš´ ìœ í˜•ì˜ url ë°œìƒ! %s %s" % (url, url2))
        except Exception as e:
            logger.error("Exception:%s", e)
            logger.error(traceback.format_exc())

        return [video_url, referer_url, vtt_url]

    @staticmethod
    def get_html_episode_content(url: str) -> str:
        if url.startswith("http"):
            html_data = LogicLinkkf.get_html(url)
        else:
            url = f"https://linkkf.app{url}"

            logger.info("get_video_url(): url: %s" % url)
            data = LogicLinkkf.get_html(url)

            tree = html.fromstring(data)

            tree = html.fromstring(data)

            pattern = re.compile("var player_data=(.*)")

            js_scripts = tree.xpath("//script")

            iframe_info = None
            index = 0

            for js_script in js_scripts:
                # print(f"{index}.. {js_script.text_content()}")
                if pattern.match(js_script.text_content()):
                    # logger.debug("match::::")
                    match_data = pattern.match(js_script.text_content())
                    iframe_info = json.loads(
                        match_data.groups()[0].replace("path:", '"path":')
                    )
                    # logger.debug(f"iframe_info:: {iframe_info}")

                index += 1

            ##################################################
            # iframe url:: https://s2.ani1c12.top/player/index.php?data='+player_data.url+'
            ####################################################

            url = f"https://s2.ani1c12.top/player/index.php?data={iframe_info['url']}"
            html_data = LogicLinkkf.get_html(url)

        return html_data

    def get_anime_info(self, cate, page):
        try:
            items_xpath = '//div[@class="ext-json-item"]'
            title_xpath = ""

            if cate == "ing":
                # url = f"{P.ModelSetting.get('linkkf_url')}/airing/page/{page}"
                # User requested to use 'anime-list' ID (categorytagid=2) for 'ing'
                url = "https://linkkf.5imgdarr.top/api/singlefilter.php?categorytagid=2&page={}&limit=20".format(page)
                items_xpath = None # JSON fetching
                title_xpath = None
            elif cate == "movie":
                # url = f"{P.ModelSetting.get('linkkf_url')}/ani/page/{page}"
                # items_xpath = '//div[@class="myui-vodlist__box"]'
                # title_xpath = './/a[@class="text-fff"]//text()'
                
                # API Spec: categorytagid=5061 (Movie)
                url = "https://linkkf.5imgdarr.top/api/singlefilter.php?categorytagid=5061&page={}&limit=20".format(page)
                items_xpath = None
                title_xpath = None

            elif cate == "complete":
                # User requested to comment out for now (25-12-31)
                # url = "https://linkkf.5imgdarr.top/api/singlefilter.php?categorytagid=2&page={}&limit=20".format(page)
                url = "" # Disable
                items_xpath = None
                title_xpath = None
            elif cate == "top_view":
                # API Spec: type=month|week|day, page=1
                url = "https://linkkf.5imgdarr.top/api/apiview.php?type=month&page={}".format(page)
                items_xpath = None # JSON fetching
                title_xpath = None
            else:
                # Default: JSON API (singlefilter)
                url = "https://linkkf.5imgdarr.top/api/singlefilter.php?categorytagid=1970&page=1&limit=20"
                items_xpath = None  # JSON fetching
                title_xpath = None

            logger.info("url:::> %s", url)
            
            if self.referer is None:
                self.referer = "https://linkkf.live"

            data = {"ret": "success", "page": page}
            response_data = LogicLinkkf.get_html(url, timeout=10)
            
            # JSON ì‘ë‹µ ì²˜ë¦¬ (Top View í¬í•¨)
            # Zendriver returns HTML-wrapped JSON: <html>...<pre>JSON</pre>...</html>
            json_text = response_data
            if response_data.strip().startswith('<html') or response_data.strip().startswith('<!'):
                try:
                    tree_temp = html.fromstring(response_data)
                    pre_content = tree_temp.xpath('//pre/text()')
                    if pre_content:
                        json_text = pre_content[0]
                except Exception:
                    pass
            
            try:
                json_data = json.loads(json_text)
                # P.logger.debug(json_data)
                
                # top_view ì²˜ë¦¬ëŠ” ë³„ë„ ë¡œì§ (êµ¬ì¡°ê°€ ë‹¤ë¦„)
                if cate == "top_view":
                    items = json_data if isinstance(json_data, list) else []
                    data["episode_count"] = len(items)
                    data["total_page"] = 100 # API limits unclear, defaulting to enough
                    data["episode"] = []
                    
                    for item in items:
                        entity = {}
                        # API: postid, postname, postthum, postdate, ...
                        entity["code"] = str(item.get("postid"))
                        entity["title"] = item.get("postname")
                        entity["image_link"] = item.get("postthum")
                        entity["link"] = f"https://linkkf.live/{entity['code']}"
                        entity["chapter"] = "Top" # Rank or simple tag
                        
                        data["episode"].append(entity)
                    return data
                
                # ê¸°ì¡´ JSON ì²˜ë¦¬ (ing ë“±)
                if isinstance(json_data, dict):
                    return json_data
                else:
                    data["episode"] = json_data if isinstance(json_data, list) else []
                    return data
            except (json.JSONDecodeError, ValueError):
                pass

            # JSON APIì¸ ê²½ìš° items_xpathê°€ Noneì´ë¯€ë¡œ, ì´ ê²½ìš° HTML íŒŒì‹± ìŠ¤í‚µ
            if items_xpath is None:
                P.logger.error("JSON parsing failed but items_xpath is None - invalid API response")
                return {"ret": "error", "log": "Invalid API response (expected JSON)"}

            tree = html.fromstring(response_data)
            tmp_items = tree.xpath(items_xpath)

            if tree.xpath('//div[@id="wp_page"]//text()'):
                data["total_page"] = tree.xpath('//div[@id="wp_page"]//text()')[-1]
            else:
                data["total_page"] = 0
            data["episode_count"] = len(tmp_items)
            data["episode"] = []

            for item in tmp_items:
                entity = dict()
                entity["link"] = item.xpath(".//a/@href")[0]
                entity["code"] = re.search(r"[0-9]+", entity["link"]).group()
                entity["title"] = item.xpath(title_xpath)[0].strip()
                entity["image_link"] = item.xpath("./a/@data-original")[0]
                entity["chapter"] = (
                    item.xpath("./a/span//text()")[0].strip()
                    if len(item.xpath("./a/span//text()")) > 0
                    else ""
                )
                # logger.info('entity:::', entity['title'])
                data["episode"].append(entity)

            # logger.debug(data)

            return data
        except Exception as e:
            P.logger.error("Exception:%s", e)
            P.logger.error(traceback.format_exc())
            return {"ret": "exception", "log": str(e)}

    def get_search_result(self, query, page, cate):
        try:
            # API URL: https://linkkf.5imgdarr.top/api/search.php
            api_url = "https://linkkf.5imgdarr.top/api/search.php"
            params = {
                "keyword": query,
                "page": page,
                "limit": 20
            }
            logger.info(f"get_search_result API: {api_url}, params: {params}")
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                "Referer": "https://linkkf.live/"
            }
            
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            result_json = response.json()
            
            data = {"ret": "success", "page": page, "episode": []}
            
            if result_json.get("status") == "success":
                items = result_json.get("data", [])
                pagination = result_json.get("pagination", {})
                
                data["total_page"] = pagination.get("total_pages", 0)
                data["episode_count"] = pagination.get("total_results", 0)
                
                for item in items:
                    entity = {}
                    entity["code"] = str(item.get("postid"))
                    entity["title"] = item.get("name")
                    
                    thumb = item.get("thumb")
                    if thumb:
                         if thumb.startswith("http"):
                             entity["image_link"] = thumb
                         else:
                             entity["image_link"] = f"https://rez1.ims1.top/350x/{thumb}"
                    else:
                        entity["image_link"] = ""
                        
                    entity["chapter"] = item.get("postnoti") or item.get("seasontype") or ""
                    entity["link"] = f"https://linkkf.live/{entity['code']}"
                    
                    data["episode"].append(entity)
            else:
                 data["total_page"] = 0
                 data["episode_count"] = 0

            return data
        except Exception as e:
            logger.error(f"Exception: {str(e)}")
            logger.error(traceback.format_exc())
            return {"ret": "exception", "log": str(e)}

    def get_series_info(self, code):
        data = {"code": code, "ret": False}
        try:
            # ì´ì „ ë°ì´í„°ê°€ ìˆë‹¤ë©´, ë¦¬í„´ (# If you have previous data, return)
            if (
                LogicLinkkf.current_data is not None
                and LogicLinkkf.current_data["code"] == code
                and LogicLinkkf.current_data["ret"]
            ):
                return LogicLinkkf.current_data
            
            url = "%s/%s/" % (P.ModelSetting.get("linkkf_url"), code)
            
            logger.info(f"get_series_info URL: {url}")

            html_content = LogicLinkkf.get_html(url, cached=False)
            
            if not html_content:
                data["log"] = "Failed to fetch page content"
                data["ret"] = "error"
                return data

            soup = BeautifulSoup(html_content, "html.parser")
            
            # === ì œëª© ì¶”ì¶œ ===
            # ë°©ë²• 1: #anime-details > h3 (ê°€ì¥ ì •í™•)
            title_elem = soup.select_one("#anime-details > h3")
            if not title_elem:
                # ë°©ë²• 2: .anime-tab-content > h3
                title_elem = soup.select_one(".anime-tab-content > h3")
            
            title_text = ""
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # "11/12 - ë„ˆì™€ ë„˜ì–´ ì‚¬ë‘ì´ ëœë‹¤" í˜•ì‹ì—ì„œ ì œëª©ë§Œ ì¶”ì¶œ
                if " - " in title_text:
                    data["title"] = title_text.split(" - ", 1)[1]
                else:
                    data["title"] = title_text
            else:
                # ë°©ë²• 3: gemini-dark-card__linkì˜ title ì†ì„±
                card_link = soup.select_one("a.gemini-dark-card__link")
                if card_link and card_link.get("title"):
                    data["title"] = card_link.get("title")
                else:
                    # ë°©ë²• 4: í¬ìŠ¤í„° ì´ë¯¸ì§€ì˜ alt ì†ì„±
                    poster_img = soup.select_one("img.gemini-dark-card__image")
                    if poster_img and poster_img.get("alt"):
                        data["title"] = poster_img.get("alt")
                    else:
                        # ë°©ë²• 5: í˜ì´ì§€ titleì—ì„œ ì¶”ì¶œ
                        page_title = soup.select_one("title")
                        if page_title:
                            title_text = page_title.get_text(strip=True)
                            # "ì œëª© ìë§‰ / ë”ë¹™ / Linkkf" í˜•ì‹ ì²˜ë¦¬
                            data["title"] = title_text.split(" ìë§‰")[0].split(" /")[0].strip()
                        else:
                            data["title"] = f"Unknown-{code}"
            
            # ì œëª© ì •ë¦¬
            data["title"] = Util.change_text_for_use_filename(data["title"]).strip()
            data["_id"] = str(code)
            
            # === ì‹œì¦Œ ì¶”ì¶œ ===
            match = re.compile(r"(?P<season>\d+)ê¸°").search(data.get("title", ""))
            if match:
                data["season"] = match.group("season")
                data["title"] = data["title"].replace(data["season"] + "ê¸°", "").strip()
            else:
                data["season"] = "1"
            
            # === í¬ìŠ¤í„° ì´ë¯¸ì§€ ===
            poster_elem = soup.select_one("img.gemini-dark-card__image")
            if poster_elem:
                # lazy loading ëŒ€ì‘: data-lazy-src (ì‚¬ì´íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì†ì„±), data-src, src ìˆœì„œë¡œ í™•ì¸
                data["poster_url"] = (
                    poster_elem.get("data-lazy-src") or 
                    poster_elem.get("data-src") or 
                    poster_elem.get("src") or ""
                )
                # placeholder SVG ì œì™¸
                if data["poster_url"].startswith("data:image/svg"):
                    data["poster_url"] = poster_elem.get("data-lazy-src") or poster_elem.get("data-src") or ""
            else:
                # ëŒ€ì•ˆ ì„ íƒì
                poster_alt = soup.select_one("a.gemini-dark-card__link img")
                if poster_alt:
                    data["poster_url"] = (
                        poster_alt.get("data-lazy-src") or 
                        poster_alt.get("data-src") or 
                        poster_alt.get("src") or ""
                    )
                else:
                    data["poster_url"] = None
            
            # === ìƒì„¸ ì •ë³´ ===
            data["detail"] = []
            info_items = soup.select("li")
            for item in info_items:
                text = item.get_text(strip=True)
                if any(keyword in text for keyword in ["ë°©ì˜ì¼", "ì œì‘ì‚¬", "ì¥ë¥´", "ë¶„ë¥˜", "ë…„"]):
                    data["detail"].append({"info": text})
            
            if not data["detail"]:
                data["detail"] = [{"ì •ë³´ì—†ìŒ": ""}]
            
            # === ì—í”¼ì†Œë“œ ëª©ë¡ - APIì—ì„œ ê°€ì ¸ì˜¤ê¸° ===
            data["episode"] = []
            data["save_folder"] = data["title"]
            
            # ì—í”¼ì†Œë“œ API í˜¸ì¶œ
            episode_api_url = f"https://linkkfep.5imgdarr.top/api2.php?epid={code}"
            try:
                episode_response = requests.get(episode_api_url, timeout=10)
                episode_data = episode_response.json()
                
                logger.debug(f"Episode API response: {len(episode_data)} servers found")
                
                # ì²« ë²ˆì§¸ ì„œë²„ (ë³´í†µ ìë§‰-S)ì˜ ì—í”¼ì†Œë“œ ëª©ë¡ ì‚¬ìš©
                if episode_data and len(episode_data) > 0:
                    server_data = episode_data[0].get("server_data", [])
                    # ì—­ìˆœ ì •ë ¬ (ìµœì‹  ì—í”¼ì†Œë“œê°€ ìœ„ë¡œ)
                    server_data = list(reversed(server_data))
                    
                    for idx, ep_info in enumerate(server_data):
                        ep_name = ep_info.get("name", str(idx + 1))
                        ep_slug = ep_info.get("slug", str(idx + 1))
                        ep_link = ep_info.get("link", "")
                        
                        # í™”ë©´ í‘œì‹œìš© titleì€ "01í™”" í˜•íƒœ
                        ep_title = f"{ep_name}í™”"
                        
                        # ì—í”¼ì†Œë“œë³„ ê³ ìœ  ID ìƒì„± (í”„ë¡œê·¸ë¨ì½”ë“œ + ì—í”¼ì†Œë“œë²ˆí˜¸)
                        episode_unique_id = data["code"] + ep_name.zfill(4)
                        
                        entity = {
                            "_id": episode_unique_id,  # ì—í”¼ì†Œë“œë³„ ê³ ìœ  ID
                            "program_code": data["code"],
                            "program_title": data["title"],
                            "save_folder": Util.change_text_for_use_filename(data["save_folder"]),
                            "title": ep_title,
                            "ep_num": ep_name,
                            "season": data["season"],
                        }
                        
                        # ì—í”¼ì†Œë“œ ì½”ë“œ = _idì™€ ë™ì¼
                        entity["code"] = episode_unique_id
                        
                        # URL ìƒì„±: playid/{code}/?server=12&slug={slug} í˜•íƒœ
                        entity["url"] = f"https://linkkf.live/playid/{code}/?server=12&slug={ep_slug}"
                        
                        # ì €ì¥ ê²½ë¡œ ì„¤ì •
                        tmp_save_path = P.ModelSetting.get("linkkf_download_path")
                        if not tmp_save_path:
                            tmp_save_path = "/tmp/anime_downloads"
                            
                        if P.ModelSetting.get("linkkf_auto_make_folder") == "True":
                            program_path = os.path.join(tmp_save_path, entity["save_folder"])
                            entity["save_path"] = os.path.normpath(program_path)
                            if P.ModelSetting.get("linkkf_auto_make_season_folder"):
                                entity["save_path"] = os.path.normpath(os.path.join(
                                    entity["save_path"], "Season %s" % int(entity["season"])
                                ))
                        else:
                            # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
                            entity["save_path"] = os.path.normpath(tmp_save_path)
                        
                        entity["image"] = data["poster_url"]
                        # filename ìƒì„± ì‹œ ìˆ«ìë§Œ ì „ë‹¬ ("01í™”" ì•„ë‹˜)
                        entity["filename"] = LogicLinkkf.get_filename(
                            data["save_folder"], data["season"], ep_name
                        )

                        # Check for existing file (for Play button)
                        entity["filepath"] = os.path.normpath(os.path.join(entity["save_path"], entity["filename"]))
                        if os.path.exists(entity["filepath"]):
                            entity["exist_video"] = True
                            if "first_exist_filepath" not in data:
                                data["first_exist_filepath"] = entity["filepath"]
                                data["first_exist_filename"] = entity["filename"]
                        else:
                            entity["exist_video"] = False
                        
                        data["episode"].append(entity)
                        
            except Exception as ep_error:
                logger.error(f"Episode API error: {ep_error}")
                logger.error(traceback.format_exc())
            
            data["episode_count"] = str(len(data["episode"]))
            data["ret"] = True
            self.current_data = data
            
            logger.info(f"Parsed series: {data['title']}, Episodes: {data['episode_count']}")
            return data

        except Exception as e:
            logger.error("Exception:%s", e)
            logger.error(traceback.format_exc())
            data["log"] = str(e)
            data["ret"] = "error"
            return data

    def get_screen_movie_info(self, page):
        try:
            url = f"{P.ModelSetting.get('linkkf_url')}/ani/page/{page}"

            html_content = self.get_html_requests(url, cached=True)
            # html_content = LogicLinkkfYommi.get_html_cloudflare(url, cached=False)
            download_path = P.ModelSetting.get("linkkf_download_path")
            tree = html.fromstring(html_content)
            tmp_items = tree.xpath('//div[@class="myui-vodlist__box"]')
            title_xpath = './/a[@class="text-fff"]//text()'
            # logger.info('tmp_items:::', tmp_items)
            data = dict()
            data = {"ret": "success", "page": page}

            data["episode_count"] = len(tmp_items)
            data["episode"] = []

            for item in tmp_items:
                entity = dict()
                entity["link"] = item.xpath(".//a/@href")[0]
                entity["code"] = re.search(r"[0-9]+", entity["link"]).group()
                entity["title"] = item.xpath(title_xpath)[0].strip()
                if len(item.xpath("./a/@style")) > 0:
                    print(
                        re.search(
                            r"url\(((http|https|ftp|ftps)\:\/\/[a-zA-Z0-9\-\.]+\.[a-zA-Z]{2,3}(\/\S*)?)\)",
                            item.xpath("./a/@style")[0],
                        ).group()
                    )

                if item.xpath(".//a/@data-original"):
                    entity["image_link"] = item.xpath(".//a/@data-original")[0]

                else:
                    entity["image_link"] = ""
                # entity["image_link"] = item.xpath("./a/@data-original")[0]
                entity["chapter"] = (
                    item.xpath("./a/span//text()")[0]
                    if len(item.xpath("./a/span//text()")) > 0
                    else ""
                )
                # logger.info('entity:::', entity['title'])
                data["episode"].append(entity)

            # json_file_path = os.path.join(download_path, "airing_list.json")
            # logger.debug("json_file_path:: %s", json_file_path)
            #
            # with open(json_file_path, "w") as outfile:
            #     json.dump(data, outfile)

            return data

        except Exception as e:
            logger.error("Exception:%s", e)
            logger.error(traceback.format_exc())


    def get_html_requests(self, url, cached=False):
        if LogicLinkkf.session is None:
            if cached:
                logger.debug("cached===========++++++++++++")

                LogicLinkkf.session = CachedSession(
                    os.path.join(self.cache_path, "linkkf_cache"),
                    backend="sqlite",
                    expire_after=300,
                    cache_control=True,
                )
                # print(f"{cache_path}")
                # print(f"cache_path:: {LogicLinkkfYommi.session.cache}")
            else:
                LogicLinkkf.session = requests.Session()

        LogicLinkkf.referer = "https://linkkf.live"

        LogicLinkkf.headers["Referer"] = LogicLinkkf.referer

        # logger.debug(
        #     f"get_html()::LogicLinkkfYommi.referer = {LogicLinkkfYommi.referer}"
        # )
        page = LogicLinkkf.session.get(url, headers=LogicLinkkf.headers)
        # logger.info(f"page: {page}")

        return page.content.decode("utf8", errors="replace")

    @staticmethod
    def get_filename(maintitle, season, title):
        try:
            # logger.debug("get_filename()===")
            # logger.info("title:: %s", title)
            # logger.info("maintitle:: %s", maintitle)
            match = re.compile(
                r"(?P<title>.*?)\s?((?P<season>\d+)ê¸°)?\s?((?P<epi_no>\d+)í™”?)"
            ).search(title)
            if match:
                epi_no = int(match.group("epi_no"))
                if epi_no < 10:
                    epi_no = "0%s" % epi_no
                else:
                    epi_no = "%s" % epi_no

                if int(season) < 10:
                    season = "0%s" % season
                else:
                    season = "%s" % season

                # title_part = match.group('title').strip()
                # ret = '%s.S%sE%s%s.720p-SA.mp4' % (maintitle, season, epi_no, date_str)
                ret = "%s.S%sE%s.720p-LK.mp4" % (maintitle, season, epi_no)
            else:
                logger.debug("NOT MATCH")
                ret = "%s.720p-SA.mp4" % maintitle

            return Util.change_text_for_use_filename(ret)
        except Exception as e:
            logger.error(f"Exception: {str(e)}")
            logger.error(traceback.format_exc())

    def add(self, episode_info):
        """Add episode to download queue with early skip checks."""
        # íê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™” (í´ë˜ìŠ¤ ë ˆë²¨ í í™•ì¸)
        if LogicLinkkf.queue is None:
            logger.warning("Queue is None in add(), initializing...")
            try:
                LogicLinkkf.queue = FfmpegQueue(
                    P, P.ModelSetting.get_int("linkkf_max_ffmpeg_process_count"), "linkkf", caller=self
                )
                LogicLinkkf.queue.queue_start()
            except Exception as e:
                logger.error(f"Failed to initialize queue: {e}")
                return "queue_init_error"
        
        # self.queueë¥¼ LogicLinkkf.queueë¡œ ë°”ì¸ë”© (í”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ ê³µìœ  ë³´ì¥)
        self.queue = LogicLinkkf.queue
        
        # 1. Check if already in queue
        if self.is_exist(episode_info):
            logger.info(f"is_exist returned True for _id: {episode_info.get('_id')}")
            return "queue_exist"
        
        # 2. Check DB for completion status FIRST (before expensive operations)
        db_entity = ModelLinkkfItem.get_by_linkkf_id(episode_info["_id"])
        
        # 3. Early file existence check - filepath is already in episode_info from get_series_info
        filepath = episode_info.get("filepath")
        if filepath:
            filepath = os.path.normpath(filepath)
        
        # ë¯¸ì™„ì„± ë‹¤ìš´ë¡œë“œ ê°ì§€ (Frag íŒŒì¼, .ytdl íŒŒì¼, .part íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ë‹¤ìš´ë¡œë“œ í—ˆìš©)
        has_incomplete_files = False
        if filepath:
            import glob
            dirname = os.path.normpath(os.path.dirname(filepath))
            has_ytdl = os.path.exists(filepath + ".ytdl")
            has_part = os.path.exists(filepath + ".part")
            has_frag = False
            if dirname and os.path.exists(dirname):
                frag_pattern = os.path.join(dirname, "*Frag*")
                has_frag = len(glob.glob(frag_pattern)) > 0
            has_incomplete_files = has_ytdl or has_part or has_frag
            
            if has_incomplete_files:
                logger.info(f"[Resume] Incomplete download detected, allowing re-download: {filepath}")
                # DB ìƒíƒœê°€ completedì´ë©´ waitë¡œ ë³€ê²½
                if db_entity is not None and db_entity.status == "completed":
                    db_entity.status = "wait"
                    db_entity.save()
        
        # DB ì™„ë£Œ ì²´í¬ (ë¯¸ì™„ì„± íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        if db_entity is not None and db_entity.status == "completed" and not has_incomplete_files:
            logger.info(f"[Skip] Already completed in DB: {episode_info.get('program_title')} {episode_info.get('title')}")
            return "db_completed"
        
        # íŒŒì¼ ì¡´ì¬ ì²´í¬ (ë¯¸ì™„ì„± íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        if filepath and os.path.exists(filepath) and not has_incomplete_files:
            logger.info(f"[Skip] File already exists: {filepath}")
            # Update DB status to completed if not already
            if db_entity is not None and db_entity.status != "completed":
                db_entity.status = "completed"
                db_entity.filepath = filepath
                db_entity.save()
            return "file_exists"
        
        # 4. Try GDM if available (like Ohli24/Anilife)
        if ModuleQueue is not None:
            entity = LinkkfQueueEntity(P, self, episode_info)
            
            # URL ì¶”ì¶œ ìˆ˜í–‰ (GDM ìœ„ì„ì„ ìœ„í•´ í•„ìˆ˜)
            try:
                entity.prepare_extra()
                if not entity.url or entity.url == entity.playid_url:
                     logger.error("Failed to extract Linkkf video URL")
                     return "extract_failed"
            except Exception as e:
                logger.error(f"Linkkf extraction error: {e}")
                return "extract_failed"

            logger.debug("entity:::> %s", entity.as_dict())
            
            # Save to DB first
            if db_entity is None:
                ModelLinkkfItem.append(entity.as_dict())
            
            # ì„¤ì •ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ë° ì“°ë ˆë“œ ìˆ˜ ì½ê¸°
            download_method = P.ModelSetting.get("linkkf_download_method") or "ytdlp"
            download_threads = P.ModelSetting.get_int("linkkf_download_threads") or 16
            
            # LinkkfëŠ” í•­ìƒ 'linkkf' source_type ì‚¬ìš© (GDMì—ì„œ YtdlpAria2Downloaderë¡œ ë§¤í•‘ë¨)
            gdm_source_type = "linkkf"

            # Prepare GDM options
            gdm_options = {
                "url": entity.url,
                "save_path": entity.savepath,
                "filename": entity.filename,
                "source_type": gdm_source_type,
                "caller_plugin": f"{P.package_name}_{self.name}",
                "callback_id": episode_info["_id"],
                "title": entity.filename or episode_info.get('title'),
                "thumbnail": episode_info.get('image'),
                "meta": {
                    "series": entity.content_title,
                    "season": entity.season,
                    "episode": entity.epi_queue,
                    "source": "linkkf"
                },
                "headers": entity.headers,
                "subtitles": entity.vtt,
                "connections": download_threads,
            }
            
            task = ModuleQueue.add_download(**gdm_options)
            if task:
                logger.info(f"Delegated Linkkf download to GDM: {entity.filename} (Method: {download_method})")
                return "enqueue_gdm_success"
        
        # 5. Fallback to FfmpegQueue if GDM not available
        logger.warning("GDM Module not found, falling back to FfmpegQueue")
        queue_len = len(self.queue.entity_list) if self.queue else 0
        logger.info(f"add() - Queue length: {queue_len}, episode _id: {episode_info.get('_id')}")
        
        if db_entity is None:
            entity = LinkkfQueueEntity(P, self, episode_info)
            logger.debug("entity:::> %s", entity.as_dict())
            ModelLinkkfItem.append(entity.as_dict())
            self.queue.add_queue(entity)
            return "enqueue_db_append"
        else:
            # db_entity exists but status is not completed
            status = db_entity.get("status") if isinstance(db_entity, dict) else db_entity.status
            logger.info(f"db_entity status: {status}, adding to queue")
            
            try:
                entity = LinkkfQueueEntity(P, self, episode_info)
                logger.info(f"LinkkfQueueEntity created, url: {entity.url}, filepath: {entity.filepath}")
                result = self.queue.add_queue(entity)
                logger.info(f"add_queue result: {result}, queue length after: {len(self.queue.entity_list)}")
            except Exception as e:
                logger.error(f"Error creating entity or adding to queue: {e}")
                logger.error(traceback.format_exc())
                return "entity_creation_error"
            
            return "enqueue_db_exist"


    # def is_exist(self, info):
    #     print(self.download_queue.entity_list)
    #     for en in self.download_queue.entity_list:
    #         if en.info["_id"] == info["_id"]:
    #             return True

    def is_exist(self, info):
        if LogicLinkkf.queue is None:
            return False
            
        for _ in LogicLinkkf.queue.entity_list:
            if _.info["_id"] == info["_id"]:
                return True
        return False

    def plugin_load(self):
        try:
            logger.debug("%s plugin_load", P.package_name)
            
            # ìƒˆ ì„¤ì • ì´ˆê¸°í™” (ê¸°ì¡´ ì„¤ì¹˜ì—ì„œ ëˆ„ë½ëœ ì„¤ì • ì¶”ê°€)
            new_settings = {
                "linkkf_notify_enabled": "False",
                "linkkf_discord_webhook_url": "",
                "linkkf_telegram_bot_token": "",
                "linkkf_telegram_chat_id": "",
            }
            for key, default_value in new_settings.items():
                if P.ModelSetting.get(key) is None:
                    P.ModelSetting.set(key, default_value)
                    logger.info(f"[Linkkf] Initialized new setting: {key}")
            
            # ì¶”ê°€ ì„¤ì •: ìë™ ë‹¤ìš´ë¡œë“œ vs ì•Œë¦¼ë§Œ
            if P.ModelSetting.get("linkkf_auto_download_new") is None:
                P.ModelSetting.set("linkkf_auto_download_new", "True")
                logger.info("[Linkkf] Initialized setting: linkkf_auto_download_new")
            
            # ëª¨ë‹ˆí„°ë§ ì£¼ê¸° ì„¤ì • (ê¸°ë³¸ 10ë¶„)
            if P.ModelSetting.get("linkkf_monitor_interval") is None:
                P.ModelSetting.set("linkkf_monitor_interval", "10")
                logger.info("[Linkkf] Initialized setting: linkkf_monitor_interval")
            
            # í´ë˜ìŠ¤ ë ˆë²¨ í ì´ˆê¸°í™”
            if LogicLinkkf.queue is None:
                LogicLinkkf.queue = FfmpegQueue(
                    P, P.ModelSetting.get_int("linkkf_max_ffmpeg_process_count"), "linkkf", caller=self
                )
                LogicLinkkf.queue.queue_start()
            
            self.queue = LogicLinkkf.queue
            self.current_data = None

            # new version Todo:
            # if self.download_queue is None:
            #     self.download_queue = queue.Queue()
            #
            # if self.download_thread is None:
            #     self.download_thread = threading.Thread(target=self.download_thread_function, args=())
            #     self.download_thread.daemon = True
            #     self.download_thread.start()

        except Exception as e:
            logger.error("Exception:%s", e)
            logger.error(traceback.format_exc())

    def plugin_unload(self):
        pass

    def scheduler_function(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ í•¨ìˆ˜ - linkkf ìë™ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬"""
        from framework import F
        logger.info("linkkf scheduler_function::=========================")
        
        # Flask ì•± ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ë³„ë„ ìŠ¤ë ˆë“œ)
        with F.app.app_context():
            try:
                content_code_list = P.ModelSetting.get_list("linkkf_auto_code_list", "|")
                auto_mode_all = P.ModelSetting.get_bool("linkkf_auto_mode_all")
                
                logger.info(f"Auto-download codes: {content_code_list}")
                logger.info(f"Auto mode all episodes: {auto_mode_all}")
                
                if not content_code_list:
                    logger.info("[Scheduler] No auto-download codes configured")
                    return
                
                # ê° ì‘í’ˆ ì½”ë“œë³„ ì²˜ë¦¬
                for code in content_code_list:
                    code = code.strip()
                    if not code:
                        continue
                        
                    if code.lower() == "all":
                        # ì‚¬ì´íŠ¸ ì „ì²´ ìµœì‹  ì—í”¼ì†Œë“œ ìŠ¤ìº”
                        logger.info("[Scheduler] 'all' mode - scanning latest episodes from site")
                        self.scan_latest_episodes(auto_mode_all)
                        continue
                    
                    logger.info(f"[Scheduler] Processing code: {code}")
                    
                    try:
                        # ì‘í’ˆ ì •ë³´ ì¡°íšŒ
                        series_info = self.get_series_info(code)
                        
                        if not series_info or "episode" not in series_info:
                            logger.warning(f"[Scheduler] No episode info for: {code}")
                            continue
                        
                        episodes = series_info.get("episode", [])
                        logger.info(f"[Scheduler] Found {len(episodes)} episodes for: {series_info.get('title', code)}")
                        
                        # ì—í”¼ì†Œë“œ ìˆœíšŒ ë° ìë™ ë“±ë¡
                        added_count = 0
                        added_episodes = []
                        for episode_info in episodes:
                            try:
                                result = self.add(episode_info)
                                if result and result.startswith("enqueue"):
                                    added_count += 1
                                    added_episodes.append(episode_info.get('title', 'Unknown'))
                                    logger.info(f"[Scheduler] Auto-enqueued: {episode_info.get('title', 'Unknown')}")
                                    self.socketio_callback("list_refresh", "")
                                    
                                # auto_mode_allì´ Falseë©´ ìµœì‹  1ê°œë§Œ (ë¦¬ìŠ¤íŠ¸ê°€ ìµœì‹ ìˆœì´ë¼ê³  ê°€ì •)
                                if not auto_mode_all and added_count > 0:
                                    logger.info(f"[Scheduler] Auto mode: latest only - stopping after 1 episode")
                                    break
                                    
                            except Exception as ep_err:
                                logger.error(f"[Scheduler] Episode add error: {ep_err}")
                                continue
                        
                        # ìƒˆ ì—í”¼ì†Œë“œ ì¶”ê°€ë¨ â†’ ì•Œë¦¼ ì „ì†¡
                        if added_count > 0:
                            self.send_notification(
                                title=series_info.get('title', code),
                                episodes=added_episodes,
                                count=added_count
                            )
                        
                        logger.info(f"[Scheduler] Completed {code}: added {added_count} episodes")
                        
                    except Exception as code_err:
                        logger.error(f"[Scheduler] Error processing {code}: {code_err}")
                        logger.error(traceback.format_exc())
                        continue
                        
            except Exception as e:
                logger.error(f"[Scheduler] Fatal error: {e}")
                logger.error(traceback.format_exc())

    def send_notification(self, title, episodes, count):
        """Discord/Telegram ì•Œë¦¼ ì „ì†¡"""
        if not P.ModelSetting.get_bool("linkkf_notify_enabled"):
            return
        
        # ë©”ì‹œì§€ ìƒì„±
        episode_list = "\n".join([f"â€¢ {ep}" for ep in episodes[:5]])
        if count > 5:
            episode_list += f"\n... ì™¸ {count - 5}ê°œ"
        
        message = f"ğŸ¬ **{title}**\nìƒˆ ì—í”¼ì†Œë“œ {count}ê°œê°€ ë‹¤ìš´ë¡œë“œ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\n\n{episode_list}"
        
        # Discord Webhook
        discord_url = P.ModelSetting.get("linkkf_discord_webhook_url")
        if discord_url:
            self.send_discord_notification(discord_url, title, message)
        
        # Telegram Bot
        telegram_token = P.ModelSetting.get("linkkf_telegram_bot_token")
        telegram_chat_id = P.ModelSetting.get("linkkf_telegram_chat_id")
        if telegram_token and telegram_chat_id:
            self.send_telegram_notification(telegram_token, telegram_chat_id, message)

    def scan_latest_episodes(self, auto_mode_all):
        """ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì—í”¼ì†Œë“œ ëª©ë¡ì„ ìŠ¤ìº”í•˜ê³  ìƒˆ ì—í”¼ì†Œë“œ ê°ì§€"""
        try:
            auto_download = P.ModelSetting.get_bool("linkkf_auto_download_new")
            
            # ìµœì‹  ë°©ì˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (1í˜ì´ì§€ë§Œ - ê°€ì¥ ìµœì‹ )
            latest_data = self.get_anime_info("ing", 1)
            
            if not latest_data or "episode" not in latest_data:
                logger.warning("[Scheduler] Failed to fetch latest anime list")
                return
            
            items = latest_data.get("episode", [])
            logger.info(f"[Scheduler] Scanned {len(items)} items from 'ing' page")
            
            total_added = 0
            all_new_episodes = []
            
            # ê° ì‘í’ˆì˜ ìµœì‹  ì—í”¼ì†Œë“œ í™•ì¸
            for item in items[:20]:  # ìƒìœ„ 20ê°œë§Œ ì²˜ë¦¬ (ì„±ëŠ¥ ê³ ë ¤)
                try:
                    code = item.get("code")
                    if not code:
                        continue
                    
                    # í•´ë‹¹ ì‘í’ˆì˜ ì—í”¼ì†Œë“œ ëª©ë¡ ì¡°íšŒ
                    series_info = self.get_series_info(code)
                    if not series_info or "episode" not in series_info:
                        continue
                    
                    episodes = series_info.get("episode", [])
                    series_title = series_info.get("title", code)
                    
                    # ìƒˆ ì—í”¼ì†Œë“œë§Œ ì¶”ê°€ (add ë©”ì„œë“œê°€ ì¤‘ë³µ ì²´í¬í•¨)
                    for ep in episodes[:5]:  # ìµœì‹  5ê°œë§Œ í™•ì¸
                        try:
                            if auto_download:
                                result = self.add(ep)
                                if result and result.startswith("enqueue"):
                                    total_added += 1
                                    all_new_episodes.append(f"{series_title} - {ep.get('title', '')}")
                                    self.socketio_callback("list_refresh", "")
                            else:
                                # ì•Œë¦¼ë§Œ (ë‹¤ìš´ë¡œë“œ ì•ˆí•¨) - DB ì²´í¬ë¡œ ìƒˆ ì—í”¼ì†Œë“œì¸ì§€ í™•ì¸
                                ep_code = ep.get("code", "")
                                existing = ModelLinkkfItem.get_by_code(ep_code) if ep_code else None
                                if not existing:
                                    all_new_episodes.append(f"{series_title} - {ep.get('title', '')}")
                            
                            if not auto_mode_all and total_added > 0:
                                break
                        except Exception:
                            continue
                    
                    if not auto_mode_all and total_added > 0:
                        break
                        
                except Exception as e:
                    logger.debug(f"[Scheduler] Error scanning {item.get('code', 'unknown')}: {e}")
                    continue
            
            # ê²°ê³¼ ì•Œë¦¼
            if all_new_episodes:
                mode_text = "ìë™ ë‹¤ìš´ë¡œë“œ" if auto_download else "ìƒˆ ì—í”¼ì†Œë“œ ê°ì§€"
                self.send_notification(
                    title=f"[{mode_text}] ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§",
                    episodes=all_new_episodes,
                    count=len(all_new_episodes)
                )
                logger.info(f"[Scheduler] 'all' mode completed: {len(all_new_episodes)} new episodes found")
            else:
                logger.info("[Scheduler] 'all' mode: No new episodes found")
                
        except Exception as e:
            logger.error(f"[Scheduler] scan_latest_episodes error: {e}")
            logger.error(traceback.format_exc())

    def send_discord_notification(self, webhook_url, title, message):
        """Discord Webhookìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡"""
        try:
            payload = {
                "embeds": [{
                    "title": f"ğŸ“º Linkkf ìë™ ë‹¤ìš´ë¡œë“œ",
                    "description": message,
                    "color": 0x10B981,  # ì´ˆë¡ìƒ‰
                    "footer": {"text": "FlaskFarm Anime Downloader"}
                }]
            }
            response = requests.post(webhook_url, json=payload, timeout=10)
            if response.status_code in [200, 204]:
                logger.info(f"[Notify] Discord ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {title}")
            else:
                logger.warning(f"[Notify] Discord ì•Œë¦¼ ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            logger.error(f"[Notify] Discord ì•Œë¦¼ ì˜¤ë¥˜: {e}")

    def send_telegram_notification(self, bot_token, chat_id, message):
        """Telegram Bot APIë¡œ ì•Œë¦¼ ì „ì†¡"""
        try:
            # Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (** -> *)
            telegram_message = message.replace("**", "*")
            
            url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
            payload = {
                "chat_id": chat_id,
                "text": telegram_message,
                "parse_mode": "Markdown"
            }
            response = requests.post(url, json=payload, timeout=10)
            result = response.json()
            if result.get("ok"):
                logger.info(f"[Notify] Telegram ì•Œë¦¼ ì „ì†¡ ì„±ê³µ")
            else:
                logger.warning(f"[Notify] Telegram ì•Œë¦¼ ì‹¤íŒ¨: {result.get('description', 'Unknown')}")
        except Exception as e:
            logger.error(f"[Notify] Telegram ì•Œë¦¼ ì˜¤ë¥˜: {e}")

    def download_thread_function(self):
        while True:
            try:
                while True:
                    logger.debug(self.current_download_count)
                    if self.current_download_count < P.ModelSetting.get_int(
                        f"{self.name}_max_download_count"
                    ):
                        break
                    time.sleep(5)

                db_item = self.download_queue.get()
                if db_item.status == "CANCEL":
                    self.download_queue.task_done()
                    continue
                if db_item is None:
                    self.download_queue.task_done()
                    continue

            except Exception as e:
                logger.error(f"Exception: {str(e)}")
                logger.error(traceback.format_exc())


class LinkkfQueueEntity(FfmpegQueueEntity):
    def __init__(self, P, module_logic, info):
        super(LinkkfQueueEntity, self).__init__(P, module_logic, info)
        self._vi = None
        self.epi_queue = None
        self.vtt = None
        self.srt_url = None
        self.headers = None
        
        # infoì—ì„œ í•„ìš”í•œ ì •ë³´ ì„¤ì •
        playid_url = info.get("url", "")
        self.filename = info.get("filename", "")
        self.quality = info.get("quality", "720p")
        self.season = info.get("season", "1")
        self.content_title = info.get("program_title", "")
        self.savepath = info.get("save_path", "")
        
        # savepathê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not self.savepath:
            default_path = P.ModelSetting.get("linkkf_download_path")
            logger.info(f"[DEBUG] linkkf_download_path from DB: '{default_path}'")
            logger.info(f"[DEBUG] info save_path: '{info.get('save_path', 'NOT SET')}'")
            logger.info(f"[DEBUG] info save_folder: '{info.get('save_folder', 'NOT SET')}'")
            
            if default_path:
                save_folder = info.get("save_folder", "Unknown")
                self.savepath = os.path.normpath(os.path.join(default_path, save_folder))
            else:
                self.savepath = "/tmp/anime_downloads"
            logger.info(f"[DEBUG] Final savepath set to: '{self.savepath}'")
        
        # filepath = savepath + filename (ì „ì²´ ê²½ë¡œ)
        self.filepath = os.path.normpath(os.path.join(self.savepath, self.filename)) if self.filename else self.savepath
        logger.info(f"[DEBUG] filepath set to: '{self.filepath}'")
        
        # playid URLì—ì„œ ì‹¤ì œ ë¹„ë””ì˜¤ URLê³¼ ìë§‰ URL ì¶”ì¶œì€ prepare_extraì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        self.playid_url = playid_url
        self.url = playid_url # ì´ˆê¸°ê°’ ì„¤ì •

    def get_downloader(self, video_url, output_file, callback=None, callback_function=None):
        """
        Factoryë¥¼ í†µí•´ ë‹¤ìš´ë¡œë” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì„¤ì •ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°©ì‹ì„ ì½ì–´ì˜µë‹ˆë‹¤.
        """
        from .lib.downloader_factory import DownloaderFactory
        
        # ì„¤ì •ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ë° ì“°ë ˆë“œ ìˆ˜ ì½ê¸°
        method = self.P.ModelSetting.get("linkkf_download_method") or "ytdlp"
        threads = self.P.ModelSetting.get_int("linkkf_download_threads") or 16
        logger.info(f"Linkkf get_downloader using method: {method}, threads: {threads}")
        
        return DownloaderFactory.get_downloader(
            method=method,
            video_url=video_url,
            output_file=output_file,
            headers=self.headers,
            callback=callback,
            callback_id="linkkf",
            threads=threads,
            callback_function=callback_function
        )

    def prepare_extra(self):
        """
        [Lazy Extraction] 
        ë‹¤ìš´ë¡œë“œ ì§ì „ì— ì‹¤ì œ ë¹„ë””ì˜¤ URLê³¼ ìë§‰ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        try:
            logger.info(f"Linkkf Queue prepare_extra starting for: {self.content_title} - {self.filename}")
            video_url, referer_url, vtt_url = LogicLinkkf.extract_video_url_from_playid(self.playid_url)
            
            if video_url:
                self.url = video_url
                # HLS ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
                self.headers = {
                    "Referer": referer_url or "https://linkkf.live/",
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
                }
                logger.info(f"Video URL extracted: {self.url}")
                
                # ìë§‰ URL ì €ì¥
                if vtt_url:
                    self.vtt = vtt_url
                    logger.info(f"Subtitle URL saved: {self.vtt}")
            else:
                # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ì‚¬ìš© (fallback)
                self.url = self.playid_url
            
            # ------------------------------------------------------------------
            # [IMMEDIATE SYNC] - Update DB with all extracted metadata
            # ------------------------------------------------------------------
            try:
                from .mod_linkkf import ModelLinkkfItem
                db_item = ModelLinkkfItem.get_by_linkkf_id(self.info.get("_id"))
                if db_item:
                    logger.debug(f"[SYNC] Syncing metadata for Linkkf _id: {self.info.get('_id')}")
                    # Parse episode number if possible for DB field
                    epi_no = None
                    try:
                        match = re.search(r"(?P<epi_no>\d+)", str(self.info.get("ep_num", "")))
                        if match:
                            epi_no = int(match.group("epi_no"))
                    except:
                        pass

                    db_item.title = self.content_title
                    db_item.season = int(self.season) if self.season else 1
                    db_item.episode_no = epi_no
                    db_item.quality = self.quality
                    db_item.savepath = self.savepath
                    db_item.filename = self.filename
                    db_item.filepath = self.filepath
                    db_item.video_url = self.url
                    db_item.vtt_url = self.vtt
                    db_item.save()
            except Exception as sync_err:
                logger.error(f"[SYNC] Failed to sync Linkkf metadata in prepare_extra: {sync_err}")

        except Exception as e:
            logger.error(f"Exception in video URL extraction: {e}")
            logger.error(traceback.format_exc())
            self.url = self.playid_url

    def download_completed(self):
        """ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì²˜ë¦¬ (íŒŒì¼ ì´ë™, DB ì—…ë°ì´íŠ¸ ë“±)"""
        try:
            logger.info(f"LinkkfQueueEntity.download_completed called for index {self.entity_id}")
            
            from framework import app
            with app.app_context():
                # DB ìƒíƒœ ì—…ë°ì´íŠ¸
                db_item = ModelLinkkfItem.get_by_linkkf_id(self.info.get("_id"))
                if db_item:
                    db_item.status = "completed"
                    db_item.completed_time = datetime.now()
                    db_item.filepath = self.filepath
                    db_item.filename = self.filename
                    db_item.savepath = self.savepath
                    db_item.quality = self.quality
                    db_item.video_url = self.url
                    db_item.vtt_url = self.vtt
                    db_item.save()
                    logger.info(f"Updated DB status to 'completed' for episode {db_item.id}")
                else:
                    logger.warning(f"Could not find DB item to update for _id {self.info.get('_id')}")
                
            # ì „ì²´ ëª©ë¡ ê°±ì‹ ì„ ìœ„í•´ ì†Œì¼“IO ë°œì‹  (í•„ìš” ì‹œ)
            # from framework import socketio
            # socketio.emit("linkkf_refresh", {"idx": self.entity_id}, namespace="/framework")
        except Exception as e:
            logger.error(f"Error in LinkkfQueueEntity.download_completed: {e}")
            logger.error(traceback.format_exc())

    def refresh_status(self):
        try:
            # from framework import socketio (FlaskFarm í‘œì¤€ ë°©ì‹)
            from framework import socketio
            
            data = self.as_dict()
            
            # /framework namespaceë¡œ linkkf_status ì´ë²¤íŠ¸ ì „ì†¡
            socketio.emit("linkkf_status", data, namespace="/framework")
            
        except Exception as e:
            logger.error(f"refresh_status error: {e}")

    def info_dict(self, tmp):
        # logger.debug('self.info::> %s', self.info)
        for key, value in self.info.items():
            tmp[key] = value
        tmp["vtt"] = self.vtt
        tmp["season"] = self.season
        tmp["content_title"] = self.content_title
        tmp["linkkf_info"] = self.info
        tmp["epi_queue"] = self.epi_queue
        
        # í…œí”Œë¦¿ì´ ê¸°ëŒ€í•˜ëŠ” í•„ë“œë“¤ ì¶”ê°€
        tmp["idx"] = self.entity_id
        tmp["callback_id"] = "linkkf"
        tmp["start_time"] = self.created_time.strftime("%m-%d %H:%M") if hasattr(self, 'created_time') and self.created_time and hasattr(self.created_time, 'strftime') else (self.created_time if self.created_time else "")
        tmp["status_kor"] = self.ffmpeg_status_kor if self.ffmpeg_status_kor else "ëŒ€ê¸°ì¤‘"
        tmp["percent"] = self.ffmpeg_percent if self.ffmpeg_percent else 0
        tmp["duration_str"] = ""
        tmp["current_pf_count"] = 0
        tmp["current_speed"] = self.current_speed if hasattr(self, 'current_speed') and self.current_speed else ""
        tmp["download_time"] = self.download_time if hasattr(self, 'download_time') and self.download_time else ""
        tmp["status_str"] = "WAITING" if not self.ffmpeg_status else ("DOWNLOADING" if self.ffmpeg_status == 5 else "COMPLETED" if self.ffmpeg_status == 7 else "WAITING")
        tmp["temp_fullpath"] = ""
        tmp["save_fullpath"] = self.filepath if self.filepath else ""
        tmp["duration"] = ""
        tmp["current_duration"] = ""
        tmp["current_bitrate"] = ""
        tmp["end_time"] = ""
        tmp["max_pf_count"] = 0
        tmp["exist"] = False
        
        return tmp

    def make_episode_info(self):
        url2s = []
        url = None

        try:
            data = LogicLinkkf.get_html_episode_content(self.url)
            tree = html.fromstring(data)

            xpath_select_query = '//*[@id="body"]/div/span/center/select/option'

            if len(tree.xpath(xpath_select_query)) > 0:
                # by k45734
                logger.debug("make_episode_info: select found")
                xpath_select_query = '//select[@class="switcher"]/option'
                for tag in tree.xpath(xpath_select_query):
                    url2s2 = tag.attrib["value"]
                    if "k40chan" in url2s2:
                        pass
                    elif "ani1c12" in url2s2:
                        pass
                    else:
                        url2s.append(url2s2)
            else:
                logger.debug("make_episode_info: else branch")

                tt = re.search(r"var player_data=(.*?)<", data, re.S)
                json_string = tt.group(1)
                tt2 = re.search(r'"url":"(.*?)"', json_string, re.S)
                json_string2 = tt2.group(1)
                ttt = "https://s2.ani1c12.top/player/index.php?data=" + json_string2
                response = LogicLinkkf.get_html(ttt)
                tree = html.fromstring(response)
                xpath_select_query = '//select[@id="server-list"]/option'
                for tag in tree.xpath(xpath_select_query):
                    url2s2 = tag.attrib["value"]
                    # if 'k40chan' in url2s2:
                    #    pass
                    # elif 'k39aha' in url2s2:
                    if "ds" in url2s2:
                        pass
                    else:
                        url2s.append(url2s2)

                # logger.info('dx: url', url)
                logger.info("dx: urls2:: %s", url2s)

                video_url = None
                referer_url = None  # dx

                for url2 in url2s:
                    try:
                        if video_url is not None:
                            continue
                        # logger.debug(f"url: {url}, url2: {url2}")
                        ret = LogicLinkkf.get_video_url_from_url(url, url2)
                        # logger.debug(f"ret::::> {ret}")

                        if ret is not None:
                            video_url = ret
                            referer_url = url2
                    except Exception as e:
                        logger.error("Exception:%s", e)
                        logger.error(traceback.format_exc())

                # logger.info(video_url)
                # return [video_url, referer_url]
                return video_url
            logger.info("dx: urls2:: %s", url2s)

            video_url = None
            referer_url = None  # dx

        except Exception as e:
            logger.error(f"Exception: {str(e)}")
            logger.error(traceback.format_exc())


class ModelLinkkfItem(db.Model):
    __tablename__ = "{package_name}_linkkf_item".format(package_name=P.package_name)
    __table_args__ = {"mysql_collate": "utf8_general_ci", "extend_existing": True}
    __bind_key__ = P.package_name
    id = db.Column(db.Integer, primary_key=True)
    created_time = db.Column(db.DateTime)
    completed_time = db.Column(db.DateTime)
    reserved = db.Column(db.JSON)
    content_code = db.Column(db.String)
    season = db.Column(db.Integer)
    episode_no = db.Column(db.Integer)
    title = db.Column(db.String)
    episode_title = db.Column(db.String)
    # linkkf_va = db.Column(db.String)
    linkkf_vi = db.Column(db.String)
    linkkf_id = db.Column(db.String)
    quality = db.Column(db.String)
    filepath = db.Column(db.String)
    filename = db.Column(db.String)
    savepath = db.Column(db.String)
    video_url = db.Column(db.String)
    vtt_url = db.Column(db.String)
    thumbnail = db.Column(db.String)
    status = db.Column(db.String)
    linkkf_info = db.Column(db.JSON)

    def __init__(self):
        self.created_time = datetime.now()

    def __repr__(self):
        return repr(self.as_dict())

    def as_dict(self):
        ret = {x.name: getattr(self, x.name) for x in self.__table__.columns}
        ret["created_time"] = self.created_time.strftime("%Y-%m-%d %H:%M:%S")
        ret["completed_time"] = (
            self.completed_time.strftime("%Y-%m-%d %H:%M:%S")
            if self.completed_time is not None
            else None
        )
        return ret

    def save(self):
        from framework import F
        with F.app.app_context():
            db.session.add(self)
            db.session.commit()

    @classmethod
    def get_by_id(cls, idx):
        from framework import F
        with F.app.app_context():
            return db.session.query(cls).filter_by(id=idx).first()

    @classmethod
    def get_by_linkkf_id(cls, linkkf_id):
        from framework import F
        with F.app.app_context():
            return db.session.query(cls).filter_by(linkkf_id=linkkf_id).first()

    @classmethod
    def append(cls, q):
        from framework import F
        with F.app.app_context():
            logger.debug(q)
            item = ModelLinkkfItem()
            item.content_code = q["program_code"]
            item.season = q["season"]
            item.episode_no = q["epi_queue"]
            item.title = q["content_title"]
            item.episode_title = q["title"]
            # item.linkkf_va = q["va"]
            item.linkkf_code = q["code"]
            item.linkkf_id = q["_id"]
            item.quality = q["quality"]
            item.filepath = q["filepath"]
            item.filename = q["filename"]
            item.savepath = q["savepath"]
            item.video_url = q["url"]
            item.vtt_url = q["vtt"]
            item.thumbnail = q.get("image", "")
            item.status = "wait"
            item.linkkf_info = q["linkkf_info"]
            item.save()

    @classmethod
    def get_paging_info(cls, count, page, page_size):
        total_page = int(count / page_size) + (1 if count % page_size != 0 else 0)
        start_page = (int((page - 1) / 10)) * 10 + 1
        last_page = start_page + 9
        if last_page > total_page:
            last_page = total_page
            
        ret = {
            "start_page": start_page,
            "last_page": last_page,
            "total_page": total_page,
            "current_page": page,
            "count": count,
            "page_size": page_size,
        }
        ret["prev_page"] = True if ret["start_page"] != 1 else False
        ret["next_page"] = (
            True
            if (ret["start_page"] + 10) <= ret["total_page"]
            else False
        )
        return ret

    @classmethod
    def delete_by_id(cls, idx):
        from framework import F
        with F.app.app_context():
            db.session.query(cls).filter_by(id=idx).delete()
            db.session.commit()
        return True

    @classmethod
    def web_list(cls, req):
        from framework import F
        with F.app.app_context():
            ret = {}
            page = int(req.form["page"]) if "page" in req.form else 1
            page_size = 30
            job_id = ""
            search = req.form["search_word"] if "search_word" in req.form else ""
            option = req.form["option"] if "option" in req.form else "all"
            order = req.form["order"] if "order" in req.form else "desc"
            query = cls.make_query(search=search, order=order, option=option)
            count = query.count()
            query = query.limit(page_size).offset((page - 1) * page_size)
            lists = query.all()
            ret["list"] = [item.as_dict() for item in lists]
            ret["paging"] = cls.get_paging_info(count, page, page_size)
            return ret

    @classmethod
    def make_query(cls, search="", order="desc", option="all"):
        from framework import F
        with F.app.app_context():
            query = db.session.query(cls)
            if search is not None and search != "":
                if search.find("|") != -1:
                    tmp = search.split("|")
                    conditions = []
                    for tt in tmp:
                        if tt != "":
                            conditions.append(cls.filename.like("%" + tt.strip() + "%"))
                    query = query.filter(or_(*conditions))
                elif search.find(",") != -1:
                    tmp = search.split(",")
                    for tt in tmp:
                        if tt != "":
                            query = query.filter(cls.filename.like("%" + tt.strip() + "%"))
                else:
                    query = query.filter(cls.filename.like("%" + search + f"%"))
            
            if option == "completed":
                query = query.filter(cls.status == "completed")
                
            if order == "desc":
                query = query.order_by(desc(cls.id))
            else:
                query = query.order_by(cls.id)
            return query
